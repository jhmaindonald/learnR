% !Rnw root = learnR.Rnw

\input{preamble}





A first step is to load the \pkg{tm} package.  This is designed for
working with a \textit{corpus} --- \textit{corpus} is the name for a
collection of documents.
\begin{Schunk}
\begin{Sinput}
library(tm)
\end{Sinput}
\end{Schunk}

\section{Creation of a Volatile Corpus}\label{sec:volatile}

\marginnote[11pt]{A pdf to text converter has taken the pdf for this
  document, and extracted the first three chapters into the
  respective files \textbf{1getStarted.txt}, \textbf{2workenv.txt},
  and \textbf{3statsExs.txt}.}

The data used is from three text files, stored in the \pkg{DAAGviz}
directory tree.  They hold text from the respective chapters 1, 2, 
and 3, from a (possibly older) version of this present document. We 
show two ways to use it to form a volatile corpus, i.e., a corpus that 
is stored in memory. The first breaks the process down into detailed
steps, while the second uses a much terser and summary approach.  

Unless a `volatile' corpus is saved separately or as part of the 
workspace, it will disappear at the end of the session. The \pkg{tm}
package offers, as an alternative, the creation of a `permanent' 
corpus; see \code{tm::PCorpus}.

\subsection*{Detailed steps}

First create paths to the files, and check that they seem correct:
\begin{fullwidth}
\begin{Schunk}
\begin{Sinput}
## Create paths to the text files, stored in the
## subdirectory "texts" of the DAAGviz package.
txdir <- system.file("texts", package="DAAGviz")
dir(txdir, pattern=".txt$")
\end{Sinput}
\begin{Soutput}
[1] "1getStarted.txt" "2workenv.txt"    "3statsExs.txt"   "4and5data.txt"  
\end{Soutput}
\begin{Sinput}
txfiles <- dir(txdir, pattern=".txt$", full.names=TRUE)
\end{Sinput}
\end{Schunk}
\end{fullwidth}

The following brings the first of these files into the workspace,
with one text string per line. The separate text strings are then
collapsed into a single character vector, with spaces at the end
of each line:
\begin{fullwidth}
\begin{Schunk}
\begin{Sinput}
## Input first file, with one text string per line
tx1 <- readLines(txfiles[1], encoding="latin1", warn=FALSE, 
                 skipNul = TRUE)
## Join the separate text strings end to end,
## and remove comon non-ASCII characters.
tx1 <- textclean::replace_non_ascii(paste(tx1, collapse=" "))
\end{Sinput}
\end{Schunk}
\end{fullwidth}

Repeat this process for files 2 and 3:
\begin{fullwidth}
\begin{Schunk}
\begin{Sinput}
tx2 <- readLines(txfiles[2], encoding="latin1", warn=FALSE, 
                 skipNul = TRUE)
tx2 <- textclean::replace_non_ascii(paste(tx2, collapse=" "))
tx3 <- readLines(txfiles[3], encoding="latin1", warn=FALSE, 
                 skipNul = TRUE)
tx3 <- textclean::replace_non_ascii(paste(tx3, collapse=" "))
\end{Sinput}
\end{Schunk}
\end{fullwidth}

Now bring the three text strings together into a corpus:
\begin{Schunk}
\begin{Sinput}
txcorp <- VCorpus(VectorSource(c(tx1, tx2, tx3)))
\end{Sinput}
\end{Schunk}

\subsection*{Creation of a corpus using \code{DirSource()}}

\marginnote{The call to \margtt{tm\_map()} is a mechanism for marking the
  document as UTF-8. The pdf to text converter creates UTF-8
  documents.  The tokenizer \margtt{scan\_takenizer} then calls
  \margtt{scan()}, but without marking the document that results as
  UTF-8, as required for use of \margtt{termDocumentMatrix()} or
  \code{termFreq()}.}
The following creates a directory source:
\begin{Schunk}
\begin{Sinput}
dirSource <- DirSource(directory=txdir,
                       pattern=".txt$")
\end{Sinput}
\end{Schunk}

Now create the corpus.  The text will be input from the files that
were identified, within the specified directory {\bf doc}:
\begin{Schunk}
\begin{Sinput}
toUTF8 <- function(x) iconv(x, to="ASCII",
                            sub = "byte")
txcorp <- Corpus(dirSource)
txcorp <- tm_map(txcorp,
     content_transformer(toUTF8))
\end{Sinput}
\end{Schunk}

\subsection*{Next steps}

A common starting point for further work is a term by document matrix.
For this, use \code{TermDocumentMatrix()}.  Or if a
document by term matrix is required, use \code{DocumentTermMatrix()}

%' \begin{marginfigure}[12pt]
%'   Alternatively, make repeated use of \margtt{tm\_map()} to
%'   operate directly on the corpus, thus:
%' <<tm-map, eval=FALSE>>=
%' @ %
%' \end{marginfigure}
Pre-processing steps prior to creating such a matrix may include
stripping away stopwords, elimination of white space, and conversion
to lower case.  These can be performed in the process of creating a
term document matrix, thus:
\begin{fullwidth}
\begin{Schunk}
\begin{Sinput}
ctl <- list(removePunctuation = list(preserve_intra_word_dashes = FALSE),
            removeNumbers = TRUE, 
            stopwords=c(stopwords("SMART"), "[1]"),
            wordLengths=c(3,Inf))
tx.tdm <- TermDocumentMatrix(txcorp, control=ctl)
\end{Sinput}
\end{Schunk}
\end{fullwidth}
\noindent
Notice the identification of \txtt{[1]}, which appears quite
frequently in the R output, as a stopword.  This omits it from the
list of terms.  Closer investigation would reveal other issues, most
because the default tokenizer\sidenote{See \margtt{help(termFreq)} for
  an example of a user-supplied tokenizer.} is not designed to handle
R code and output.

Stopwords are words that are likely to be uniformative for purposes
of comparing text content.  The function \code{tm::stopwords()} 
accepts the arguments \code{kind="en"}, which gives a relatively
restricted list of English stopwords, or \code{kind="SMART"} which
gives a much more extensive list.  See \code{?tm::stopwords} for
details of non-English stopword lists that are immediately available.
The following will give an idea of the sorts of words that are in
the two lists:
\begin{Schunk}
\begin{Sinput}
## First few stopwords in the "en" set
sort(stopwords())[1:5]
\end{Sinput}
\begin{Soutput}
[1] "a"     "about" "above" "after" "again"
\end{Soutput}
\begin{Sinput}
## First few stopwords in the "SMART" set 
stopwords('SMART')[1:5]
\end{Sinput}
\begin{Soutput}
[1] "a"     "a's"   "able"  "about" "above"
\end{Soutput}
\begin{Sinput}
## Stopwords in "SMART" but not in "en"; first 10
stopwords("SMART")[!stopwords("SMART")%in%stopwords()][1:10]
\end{Sinput}
\begin{Soutput}
 [1] "a's"         "able"        "according"   "accordingly" "across"     
 [6] "actually"    "afterwards"  "ain't"       "allow"       "allows"     
\end{Soutput}
\end{Schunk}

Now list terms that occur 100 or more times:
\begin{fullwidth}

\end{fullwidth}

\subsection*{Wordclouds}

First load the \pkg{wordcloud} package:
\begin{Schunk}
\begin{Sinput}
library(wordcloud)
\end{Sinput}
\end{Schunk}

Figure \ref{fig:wc} shows wordcloud plots for the first (chapter 1),
second (2) and third (4 and 5) documents in the corpus.
\vspace*{15pt}

\begin{figure*}
\begin{Schunk}


\centerline{\includegraphics[width=0.92\textwidth]{figs/10-wordcloud1-3-1} }

\end{Schunk}
\caption[][-12pt]{Wordcloud plots are A: for the words in Chapter 1; B: 2; and C: 4 - 6.\label{fig:wc}}
\setfloatalignment{b}
\end{figure*}

\vspace*{-9pt}

\noindent
Code for the plots is:
\begin{fullwidth}
\begin{Schunk}
\begin{Sinput}
pal <- brewer.pal(6, "Dark2")
fnam <- as.matrix(tx.tdm)[,1]
wordcloud(names(fnam), fnam, max.words=50, colors=pal[-1],
          random.order=FALSE, scale=c(7.5,.5))
mtext(side=3, line=3.5, "Ch 1: Basics of R", adj=0, cex=1.8)
fnam2 <- as.matrix(tx.tdm)[,2]
wordcloud(names(fnam2), fnam2, max.words=80, colors=pal[-1],
          random.order=FALSE, scale=c(5.5,.5))
mtext(side=3, line=3.5, "Ch 2: R Environment", adj=0, cex=1.8)
fnam3 <- as.matrix(tx.tdm)[,3]
wordcloud(names(fnam3), fnam3, max.words=80, colors=pal[-1],
          random.order=FALSE, scale=c(9,.8))
mtext(side=3, line=3.5, "Chs 4,5: Data . . .", adj=0, cex=1.8)
\end{Sinput}
\end{Schunk}
\end{fullwidth}
\marginnote[12pt]{All three panels used a 5in by 5in graphics page,
  with a pdf pointsize of 12.} Less frequent words will be lost off the
edge of the plot if the size of the graphics page is too small
relative to the pointsize.  Note the different scaling ranges used in
the three cases, with the large scaling range for Panel C
(\code{scale=c(9,.8)}) used to accommodate a frequency distribution
in which one item (`data') is a marked outlier.

\section{Creation of a Corpus from PDF Files}

The \pkg{tm} package has functions that can be used to create readers
for several different types of files.  Type \code{getReaders()} to get
a list.  Note in particular \code{readPDF()} that can be used with pdf
files.  See \code{?tm::readPDF} for details of PDF extraction
engines that may be used.  The default is to use the Poppler PDF
rendering library as provided in the {\bf pdftools} package.

The following sets the path to the directory {\bf pdf} in the 
package {\bf DAAGviz} that holds the pdf files for (possibly,
an older version) of the four ranges of chapters of the present
text:
\begin{Schunk}
\begin{Sinput}
uri <- system.file("pdf", package="DAAGviz")
## Check names of files in directory
dir(uri)
\end{Sinput}
\begin{Soutput}
[1] "3models.pdf" "ch1.pdf"     "ch4-5.pdf"   "ch6.pdf"    
\end{Soutput}
\end{Schunk}

The corpus that has all three documents is, starting with the
pdf files, most easily created thus:
\begin{fullwidth}
\begin{Schunk}
\begin{Sinput}
fromPDF <- Corpus(DirSource(directory=uri, pattern=".pdf$"),
                 readerControl=list(reader=readPDF,
                 PdftotextOptions = "-layout"))
makeChar <- function(x)gsub("[^[:alnum:][:blank:]]","" , x, ignore.case = TRUE)
fromPDF <- tm_map(fromPDF, content_transformer(makeChar))
\end{Sinput}
\end{Schunk}
\end{fullwidth}

Create the term-document matrix thus:
\begin{Schunk}
\begin{Sinput}
txx.tdm <- TermDocumentMatrix(fromPDF, control=ctl)
\end{Sinput}
\end{Schunk}

\section{Document Collections Supplied With \pkg{tm}}
Several document collections are supplied with the package,
as text files or as XML files.  To get the path to the
subdirectories where these document collections are stored, 
type:
\begin{fullwidth}

\begin{Schunk}
\begin{Sinput}
(pathto <- system.file("texts", package="tm"))
\end{Sinput}
\begin{Soutput}
[1] "/Library/Frameworks/R.framework/Versions/3.5/Resources/library/tm/texts"
\end{Soutput}
\begin{Sinput}
dir(pathto)
\end{Sinput}
\begin{Soutput}
[1] "acq"               "crude"             "custom.xml"       
[4] "loremipsum.txt"    "rcv1_2330.xml"     "reuters-21578.xml"
[7] "txt"              
\end{Soutput}
\end{Schunk}

\end{fullwidth}
The subdirectory \textbf{acq} has 50 Reuters documents in XML format,
\textbf{crude} has the first 23 of these, and \textbf{txt} has a
small collection of 5 text documents from the Roman poet Ovid.
These can be accessed and used for experimentation with the abilities
provided in \pkg{tm}, as required.

The following are the names of the five Ovid documents:
\begin{fullwidth}

\begin{Schunk}
\begin{Sinput}
dir(paste(pathto, "txt",sep="/"))
\end{Sinput}
\begin{Soutput}
[1] "ovid_1.txt" "ovid_2.txt" "ovid_3.txt" "ovid_4.txt" "ovid_5.txt"
\end{Soutput}
\end{Schunk}

\end{fullwidth}

The following brings these documents into a volatile corpus, i.e.,
a corpus that is stored in memory:
\begin{Schunk}
\begin{Sinput}
(ovid <-
   Corpus(DirSource(paste(pathto, "txt", sep="/")),
          readerControl=list(language="lat")))
\end{Sinput}
\begin{Soutput}
<<SimpleCorpus>>
Metadata:  corpus specific: 1, document level (indexed): 0
Content:  documents: 5
\end{Soutput}
\end{Schunk}

