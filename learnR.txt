8
Regression with Linear Terms and Factors
146 learning and exploring r

Linear Models, in the style of lm():

 Linear model Any model that lm() will ﬁt is a “linear” model.
                     lm() can ﬁt highly non-linear forms of response!

Diagnostic  Use plot() with the model object as argument,
plots       to get a basic set of diagnostic plots.

termplot() If there are no interaction terms, use termplot()
                   to visualize the contributions of the diﬀerent terms.

Factors     In model formulae, factors model qualitative eﬀects.

Model       The model matrix shows how coeﬃcients should be
matrices    interpreted. (This is an especial issue for factors.)

GLMs        Generalized Linear Models are an extension of
            linear models, commonly used for analyzing counts.

Modern      This can use smoothers – spline and other functions
regession   of explanatory variables that adapt to suit the data.

GAMs        Generalized Additive Models extend linear models
            to allow smoothing curves and surfaces.

[NB: lm() assumes independently & identically distributed (iid)
errors, perhaps after applying a weighting function.]

    In this chapter, the chief focus will be on the lm() (linear model)   Thus spline ﬁts are formed as a linear
function, discussed earlier in Section 3.4. The lm() function is the      combination from a kitset of curves.
most widely used of a huge range of model ﬁtting abilities, available
in the various R packages.

    Linear models are linear in the model parameters, not necessarily
in the variables. A linear model can perfectly well ﬁt a combination
of basis curves.

8.1 Linear Models in R – Basic Ideas                                      1 The standard errors become in-
                                                                          creasingly unrealistic as the number
Here, we ﬁt a straight line, which is very obviously a linear model!      of possible choices of model terms
This simple starting point gives little hint of the range of models that  (variables, factors and interactions)
can be ﬁtted using R’s linear model lm() function.                        increases.

    The lm() function returns, as well as estimates, standard errors      2 Wilkinson, GN and Rogers, CE,
for parameters and for predictions. The standard error and p-value        1973. Symbolic description of models
information provided by the lm() function assumes that the random         in analysis of variance, Applied
term is i.i.d. (independently and identically distributed) normal. The    Statistics 22: 392-399.
independence assumption can be crucial.

    The standard errors assume, also, that the analysis is based on a
model was chosen in advance.1 If this is not the case, it can be im-
portant to resort to the use of empirical methods for assessing model
performance – training/test methodology, or cross-validation,or the
bootstrap.

    The symbolic notation2 that is available in R for describing lin-
                                                        regression with linear terms and factors 147

ear models makes it straightforward to set up quite elaborate and
intricate models.

Scatterplot with ﬁtted line – an example                                                   30 q

The following plots data from the data frame roller (as in Figure                          25                                   q
8.1) from the DAAG package.                                                                                                  q
                                                                               depression
library(DAAG)                                                                              20 q q
plot(depression ~ weight, data=roller, fg="gray")
                                                                                           15
    The formula depression ~ weight can be used either as a
graphics formula or as a model formula. The following ﬁts a straight                       10 q
line, then adding it to the above plot:
                                                                                           5         qq     6                   8 10 12
plot(depression ~ weight, data=roller, fg="gray")                                               q   q
roller.lm <- lm(depression ~ weight , data=roller)
# For a line through the origin, specify                                                   0             4
# depression ~ 0 + weight
abline(roller.lm)                                                                                2

Figure 8.2 repeats the plot, now with a ﬁtted line added.                                                   weight
    The diﬀerent explanatory variables in the model are called
                                                                               Figure 8.1: Plot of depression
terms. In the above, there is one explicit term only on the right,             versus weight, using data from the
i.e., weight. This is in addition to the intercept, which is included          data frame roller in the DAAG
by default.                                                                    package.

                                                                                           30 q

                                                                                           25                                   q
                                                                                                                             q
                                                                               depression
                                                                                           20 q q

                                                                                           15

                                                                                           10 q

8.1.1 Straight line regression – algebraic details                                         5         qq
                                                                                                q   q
                                                                                                            6                   8 10 12
                                                                                           0             4

                                                                                                 2

The standard form of simple straight line model can be written                                              weight

                depression = α + β × weight + noise.                           Figure 8.2: This repeats Figure 8.1,
Now write y in place of depression and x in place of weight, and               now adding a ﬁtted line.

add subscripts, so that the observations are: (x1, y1), (x2, y2), . . . ,(xn,
yn). Then the model can be written:

                   yi = α + βxi + εi.

The α + βxi term is the “ﬁxed” component of the model, and εi is
the random noise.

The line is chosen so that the sum of squares of residuals is as

small as possible, i.e., the intercept α and the slope β chosen to mini-

mize               n

                     (yi − α − βi xi)2

                                 i=1

The R function lm() will provide estimates a of α and b of β.

The straight line

                   y = a + bx

can then be added to the scatterplot.

Fitted or predicted values, calculated so that they lie on the esti-

mated line, are obtained using the formula:

                    y1 = a + bx1, y2 = a + bx2, . . . .
The residuals, which are the diﬀerences between the observed and

ﬁtted values, give information about the noise.

                   e1 = y1 − y1, e2 = y2 − y2, . . . .          (8.1)
148 learning and exploring r

8.1.2 Syntax – model, graphics and table formulae:

The syntax for lm() models that will be demonstrated here is used,
with modiﬁcation, throughout the modeling functions in R. A very
similar syntax can be used for specifying graphs and for certain
types of tables.

Model objects                                                           Components of model objects can
                                                                        be accessed directly, as list objects.
The following code returns a model object to the command line.          But it is usually better to use an
                                                                        extractor function. Note in particular
lm(depression ~ weight, data=roller)                                    residuals() (can be abbreviated
                                                                        to resid()), coefficients()
Call:                                                                   (coef()), and fitted.values()
lm(formula = depression ∼ weight, data = roller)                        (fitted()). For example:

                                                                          coef(roller.lm)

Coefficients:     weight
(Intercept)          2.67

           -2.09

When returned to the command line in this way, a printed summary
is returned.

    Alternatively, the result can be saved as a named object, which is
a form of list.

roller.lm <- lm(depression ~ weight , data=roller)

    The names of the list elements are:

names(roller.lm)

[1] "coefficients" "residuals"  "effects"
                                "qr"
"rank"                          "terms"

[5] "fitted.values" "assign"

"df.residual"

[9] "xlevels"     "call"

"model"

8.1.3 Matrix algebra – straight line regression example

In order to write the quantity

                                          10

                                 (yi − a − bxi)2

                                       i=1

that is to be minimized in matrix form, set:
                                                                                                                                                                                                                                                                                      regression with linear terms and factors 149

      1                                               1.9                                                         2                                                                                                                  2 − (a + 1.9b) 
                                                                            
X  =                                                1  3.1                                                 ;  y  =                                                1                                                 ;  e  =  y − Xb  =                                                 1 − (a + 3.1b)                                                 where b =  a
                                                    1  3.3                                                                                                        5                                                                                                                    5 − (a + 3.3b)                                                            b
                                                    1  4.8                                                                                                        5                                                                                                                    5 − (a + 4.8b)
                                                    1  5.3                                                                                                        20                                                                                                                  20 − (a + 5.3b)
                                                    1  6.1                                                                                                        20                                                                                                                  20 − (a + 6.1b)
                                                    1  6.4                                                                                                        23                                                                                                                  23 − (a + 6.4b)
                                                    1  7.6                                                                                                        10                                                                                                                  10 − (a + 7.6b)
                                                    1  9.8                                                                                                        30                                                                                                                  30 − (a + 9.8b)
                                                    1  12.4                                                                                                       25                                                                                                                  25 − (a + 12.4b)

    Here a and b are chosen to minimize the sum of squares of ele-
ments of e = y − Xb, i.e., to minimize

                                                                                                              e e = (y − Xb) (y − Xb)

The least squares equations can be solved using matrix arithmetic.

Recap, and Next Steps in Linear Modeling

For this very simple model, the model matrix had two columns only.
Omission of the intercept term will give an even simpler model
matrix, with just one column.

    Regression calculations in which there are several explanatory
variables are handled in the obvious way, by adding further columns
as necessary to the model matrix. This is however just the start to the
rich range of possibilities that model matrices open up.

8.1.4 A note on the least squares methodology                                                                                                                                                                                                                                                           The assumptions of independence and
                                                                                                                                                                                                                                                                                                        identical distribution (iid) are crucial.
More fundamental than least squares is the maximum likelihood                                                                                                                                                                                                                                           The role of normality is commonly
principle. If the “error” terms are independently and identically                                                                                                                                                                                                                                       over-stated.
normally distributed, then least squares and maximum likelihood are
equivalent.                                                                                                                                                                                                                                                                                             3 Weighted least squares is however
                                                                                                                                                                                                                                                                                                        justiﬁed by maximum likelihood if it
    Least squares will not in general yield maximum likelihood                                                                                                                                                                                                                                          is known how the variances change
estimates, and the SEs returned by lm() or by predict() from an                                                                                                                                                                                                                                         with xi, or if the pattern of change
lm model will be problematic or wrong if:                                                                                                                                                                                                                                                               can be inferred with some reasonable
                                                                                                                                                                                                                                                                                                        conﬁdence.
• Variances are not homogeneous3;
                                                                                                                                                                                                                                                                                                        Simplifying the model, in ways
• Observations are not independent;                                                                                                                                                                                                                                                                     that do not much aﬀect coeﬃcients
                                                                                                                                                                                                                                                                                                        that remain in the model, may be
• The sampling distributions of parameter estimates are noticeably                                                                                                                                                                                                                                      acceptable.
   non-normal.

• Model terms (variables, factors and/or interactions) have been
   chosen from some wider set of possibilities (the theory assumes a
   specﬁc known model).

    Normality of the model ’errors’ is more than is in practice re-
quired. Outliers, and skewness in the distribution, do often mean that
the theory cannot be satisfactorily used as a good approxation.
150 learning and exploring r

8.2 Checks — Before and After Fitting a Line

Consider here a female versus male comparison of record times for
Northern Island hill races.

  A: Untransformed scales                               q                                  B: Logarithmic scales                                                     q  Figure 8.3: Graphs compare female
                                                                                                                                                                        with male record times, for Northern
6                                                                                      5.0                                                                              Ireland hill races. Least squares lines
                                                                                                                                                                        are added, and marginal boxplots are
Female times5                                                                                                                                                           shown on the horizontal axis. Panel
                                                             Female times (log scale)                                                                                   A has untransformed scales, while
4                                                                                                                                                                 q     Panel B has log transformed scales.
                                                                                                                                                                        For the code, see the script ﬁle for this
                                                                                       2.0 q                                                                            chapter.

3                                                                                                             qq

                                              q                                        1.0             q
                                                                                               qqq
2q
                                                                                                          qq
                       qq                                                                      qqqqqqqqqqq

1 qqqqqqqqqqqq q                                                                       0.5

         q                                                                                  q

                           qq                              q                                                                                            qq           q

                          time                                                                               time
   0.5 1.0 1.5 2.0 2.5 3.0 3.5 4.0                                                          0.5 1.0 2.0

                           Male times                                                       Male times (log scale)

Untransformed vs transformed scales: Figure 8.3 shows two alter-
native views of the data. Least squares line have in each case been
added.

    In Panel A, a single data point at the top right lies well away
from the main body of data. In Panel B, points are more evenly
spread out, though still with a tail out to long times.

    The following ﬁts a regression line on the untransformed scale:

mftime.lm <- lm(timef ~ time, data=nihills)

    The line appears to ﬁt the data quite reasonably well. Is this an
eﬀective way to represent the relationship? An obvious problem is
that data values become increasingly sparse as values increase, with
one point widely separated from other data. That one data point,
widely separated from other points, stands to have a disproportionate
eﬀect in determining the ﬁtted line.

    The coeﬃcients for the line that is ﬁtted on a logarithmic scale
is:

mflogtime.lm <- lm(log(timef) ~ log(time),
                                  data=nihills)

round(coef(mflogtime.lm), 3)

(Intercept) log(time)

   0.267                                         1.042

The coeﬃcient of 1.042 for log(time) implies that the relative rate
of increase of female times is 4.2% greater than the relative rate of
increase of male times.
                                                                             regression with linear terms and factors 151

The use of residuals for checking the ﬁtted line:

In Figure 8.3, departures from the line do not stand out well rela-
tive to the line. To make residuals stand out, Figures 8.4A and 8.4B
rotate the lines, for the untransformed and transformed data respec-
tively, ∼45◦ clockwise about its mid-point, to the horizontal.

   A: Residuals, unlogged data                                                    B: Residuals, logged data               Figure 8.4: In Panel A, residuals from
                                                                                                                          the line for the unlogged data have
0.2 q                                                                      0.10              q                         q  been plotted against male times. Panel
resid(mftime.lm)                                                                                                          B repeats the same type of plot, now
                                                      resid(mflogtime.lm)0.1qqqqq q          q    q          q            for the regression for the logged data.
 0.0    qqqqq q                                                                              qqq                    q
−0.1                                                                       0.05                                           A residual of -0.1 denotes a time that
                                                                                                                          is about 10% (more accurately 9.5%)
                                                                                          q          q                    less than the ﬁtted value on the line. A
                                                                                                                          residual of 0.1 denotes a time that is
      qq             q                                                      0.00             qqq                          about 10% (more accurately 10.5%)
                                                                           −0.05                                          more than the ﬁtted value.
      q          qq                                                                            q  qq
      q                                                                                      q        q

−0.2             q

−0.3                                                                       −0.10                  qq
         0.5                                                                                           q

                              q                                                                           q

                     1.5 2.5     3.5                                              −1.0 0.0 0.5 1.0

                        time                                                                      log(time)

Notice that, in Figure 8.4A, all residuals except that for the largest
time lie very nearly on a line. The point with the largest ﬁtted value,
with the largest male (and female) time, is pulling the line out of
whack. There is a mismatch between the data and the model. The
picture in Figure 8.4B is much improved, though there still is an
issue with the point for the longest time.

    Residuals on the vertical scale of Figure 8.4B are on a scale
of natural logarithms (logarithms to base e). As the range is small
(roughly between -0.1 and 0.1), the values can be interpreted as
relative diﬀerences on the scale of times.

The common beneﬁts of a logarithmic transformation: Where
measurement data have a long tail out to the right, it commonly
makes sense to work with logarithms of data values, as in Figure
8.3B. Often, working with data on a logarithmic scale has several
useful consequences:

- The skewness is reduced

- The variation at the high end of the range of values is reduced,
   relative to variation at the low end of the range of values.

- Working on a logarithmic scale is equivalent to working with
   relative, rather than absolute, change. Thus a change from 10 to
   20 is equivalent to a change from 20 to 40, or from 40 to 80. On a
  logarithmic scale, these are all changes by an amount of log(2).

- By default, the function log()) returns natural logarithms, i.e.,
   logarithms to base e. On this scale, a change of 0.05 is very close
   to a change of 5%. A change of 0.15 is very roughly a change
   of 15%. [A decrease of 0.15 is a decrease of 13.9%, while an
   increase of 0.15 is a increase of 16.2%]
152 learning and exploring r                                                             4 Statistics that try to provide an
                                                                                         overall evaluation focus too much on
    Once the model is ﬁtted, checks can and should be made on the                        a speciﬁc form of departure, and do
extent and manner of diﬀerences between observations and ﬁtted                           a poor job at indicating whether the
model values. Graphical checks are the most eﬀective,4 at least                          departure from assumptions matters.
as a starting point. Mostly, such checks are designed to highlight
common types of departure from the model.                                                Section 8.3 demonstrates the use of
                                                                                         simulation to help in judging between
    Figure 8.4B provided a simple form of diagnostic check. This                         genuine indications of model depar-
is one of several checks that are desirable when models have been                        tures and features of the plots that may
ﬁtted.                                                                                   well reﬂect statistical variation.

8.2.1 ∗Diagnostics – checks on the ﬁtted model

For lm models, the R system has a standard set of diagnostic plots
that users are encouraged to examine. These are a starting point for
investigation. Are apparent departures real, or may they be a result
of statistical variation? For the intended use of the model output, do
apparent departures from model assumptions matter.

    For drawing attention to diﬀerences between the data and what
might be expected given the model, plots that show residuals are in
general much more eﬀective than plots that show outcome (y) vari-
able values. Additionally, plots are needed that focus on common
speciﬁc types of departure from the model.

All four diagnostic plots

Figure 8.5 shows the default diagnostic plots for the regression with
the untransformed data:

      Residuals vs Fitted                       Normal Q−Q                  Scale−Location                                                                                                  Residuals vs Leverage

                                        4 Seven Sevensq             2.0 Seven Sevensq                                                                                                      4 Seven Sevensq
                                                                                                  qAnnalong Horseshoe
                                        2
                                        0 qqqqqqqqqqqqqqqqqqq q     1.5
                                       −2 qDonard & Commedagh                        qDonard & Commedagh
Residuals
                                                      Standardized residualsqAnnalong Horseshoe
                                                                                                             Standardized residuals−2 −1 0 1 2
                                                                                                                                                                   Standardized residuals
0.2 Seven Sevensq                            Theoretical Quantiles

 0.1  qqqqqqqqqqqq  q                                               1.0 q                                                                                                                  2                       01.5
 0.0    qq                                                                  qqqq qqq                                                                                                       0 qqqqqqqqqqqq          01.5
−0.1      qqqq                                                                  q

−0.2  qDonard & Commedagh                                           0.5 qqqqqqq       q                                                                                                    −2 qDonard & Commedagh
−0.3              qAnnalong Horseshoe
                                                                                                                                                                                                      q AnnCaloonogkH'sorsdeisshtoaence
      12345                                                         0.0                                                                                                                    −4

         Fitted values                                                         12345                                                                                                           0.0 0.2 0.4 0.6 0.8

                                                                                Fitted values                                                                                                            Leverage

Simpliﬁed code is:                                                                       Figure 8.5: Diagnostic plots from the
                                                                                         regession of timef on time.
mftime.lm <- lm(timef ~ time, data=nihills)
plot(mftime.lm , cex.caption=0.8)                                                        Two further plots are available; specify
                                                                                         which=4 or which=6, e.g.
The ﬁrst of these plots has similar information to Figure 8.4A above.
A diﬀerence is that residuals are now plotted against ﬁtted values. It                     plot(mftime.lm, which=4.
is immaterial, where there is just one explanatory variable, whether                     These give a diﬀerent slant on what
residuals are plotted against ﬁtted values or against x-values – the                     is shown in the fourth default plot
diﬀerence between plotting against a + bx and plotting against x                         (which=5).
amounts only to a change of labeling on the x-axis.

    Figure 8.6 shows the default diagnostic plots for the transformed
data. Simpliﬁed code is:
                                                                                                              regression with linear terms and factors 153

plot(mflogtime.lm , cex.caption=0.8)
par(opar)

               Residuals vs Fitted                                Normal Q−Q                                         Scale−Location                                                            Residuals vs Leverage

                                                                                             Seven Sevensq    1.5 Seven Sevensq
Residuals                 Seven Sevensq                   2                                                                                                                                              Seven Sevensq
0.10                                                  Standardized residualsq
                                                                                                             Standardized residualsq                                                                                             1
                                                                                                                                                                   Standardized residuals                                        0.5
                                                          1 qqqqqq                                                                  qDonard & Commedagh                                    2
                                                          0 qqqqq                                                    q qSlieve Bearnagh                                                               q

0.05           qqqq q     q                                                        qqq                               q  qq                                                                 1 qqqqq q
                                                                                qq                                   qqq
                       q                                 −1 q                                                 1.0                  q     qq
                                                                         qq                                            q
            q  qqq                                                    qSlieve Bearnagh                                                q                                                        q
 0.00           qqq                                      −2 qDonard & Commedagh                                                                                                                qqq
−0.05          qq                                             −2 −1 0 1 2                                                                                                                  0        q

                                                               Theoretical Quantiles                          0.5 q  q qq q                                                                    qqq
                                                                                                                                                                                               qq
                             q                                                                                                                                                                           qAnnalong Horseshoe 0.5

−0.10             qq                                                                                                         q                                                             −1  qq                                   1
                      q SlievqeDBoenaarrndag&hCommedagh                                                                        qq                                                          −2
                                                                                                                                                                                               q q DonaCrdo&oCko'ms mdeisdataghnce
                                                                                                              0.0

−1.0 0.0 0.5 1.0 1.5                                                                                          −1.0 0.0 0.5 1.0 1.5                                                             0.0 0.1 0.2 0.3 0.4

          Fitted values                                                                                                 Fitted values                                                                    Leverage

The point for the largest time still has a very large leverage and, as                                                                       Figure 8.6: Diagnostic plots from
indicated by its position relative to the Cook’s distance contours,                                                                          the regression of log(timef) on
a large inﬂuence on the ﬁtted regression line. This may, in part or                                                                          log(time).
entirely, be a result of a variance that, as suggested by the scale-
location plot in Panel 3, tends to increase with increasing ﬁtted
value. The normal Q-Q plot (Panel 2) suggests an overall distri-
bution of residuals that is acceptably normal. The large residual
associated with the largest time in Panel 1 would look much less out
of place if there was an adjustment that allowed for a variance that
increases with increasing ﬁtted value.

    These diﬀerences from the assumed model are small enough that,
for many purposes, the line serves as a good summary of the data.
The equation is:

mflogtime.lm <- lm(log(timef) ~ log(time),
                                  data=nihills)

round(coef(mflogtime.lm), 3)

(Intercept) log(time)

0.267                           1.042

The coeﬃcient that equals 1.042 can be interpreted as a relative rate                                                                        To obtain this second plot only,
of increase, for female time relative to male time. Consider a race                                                                          without the others, type:
for which the male record time is 100 minutes. The predicted female
time is:                                                                                                                                     plot(mftime.lm , which=2)

       exp(0.267 + 1.042 log(100/60)) = 2.223h = 133.4m.

An increase of one minute, or 1%, in the male time, is predicted to
lead to an increase of close to 1.042 × 1% in the female time. The
predicted increase is 133.4 × 1.042m.

Panel 2 — A check for normality: The second panel in Figure 8.5
identiﬁes two large negative residuals and one large positive resid-
ual. This seems inconsistent with the assumption that residuals have
a normal dsitribution. Again, Figure 8.6 shows an improvement.

    Modest departures from normality are not a problem per se.
Heterogeneity of variance, and outliers in the data, are likely to be
the more serious issues.
154 learning and exploring r

Panel 3 — Is the variance constant?: The third panel is designed        To obtain this third plot only, without
to check whether variation about the ﬁtted line, as measured by the     the others, type:
variance, is constant. For this, there should be no trend up or down
in the points. The large upward trend in the third panel of 8.5 has     plot(mftime.lm , which=3)
largely disappeared in the third panel of Figure 8.6.
                                                                        To obtain this fourth plot only, with-
Panel 4 — a check for high leverage points: The fourth panel is         out the others, type:
designed to check on points with large leverage and/or large inﬂu-
ence. In straight line regression, the most extreme leverage points     plot(mftime.lm , which=5)
are points that are separated from the main body of points, and are at
the high or (less commonly) low end of the range of x-values.           For working within the main range
                                                                        of the data values, we might prefer
    The combined eﬀect of leverage and magnitude of residual            to use the regression line that is
determines what inﬂuence a point has. Large leverage translates         obtained when ‘Seven Sevens’ is
into large inﬂuence, as shown by a large Cook’s distance, when the      omitted. If ‘Seven Sevens’ is omitted
residual is also large. Points that lie within the region marked out    in estimating the regression line, this
by the 0.5 or (especially) the 1.0 contour for Cook’s distance have     should be made clear in any report,
a noticeable inﬂuence on the ﬁtted regression equation. Even with       and the reason explained. The large
the logged data, the point for the largest time (‘Seven Sevens’) is     residual for this point does hint that
skewing the regression line noticeably.                                 extrapolation much beyond the upper
                                                                        range of data values is hazardous.
    Note that there is no reason to suspect any error in this value.
Possibly the point is taking us into a part of the range where the
relationship is no longer quite linear. Or this race may be untypical
for more reasons than that it is an unusually long race.

    The following shows the change when ‘Seven Sevens’ is omit-
ted:

round(coef(mflogtime.lm), 4)

(Intercept) log(time)

0.2667         1.0417

omitrow <- rownames(nihills)!="Seven Sevens"
update(mflogtime.lm , data=subset(nihills , omitrow))

Call:
lm(formula = log(timef) ∼ log(time), data = subset(nihills , omitrow))

Coefficients:

(Intercept) log(time)

0.239          0.991

8.2.2 The independence assumption is crucial

A key assumption is that observations are independent. The indepen-
dence assumption is an assumption about the process that generated
the data, about the way that it should be modeled. There is no one
standard check that is relevant in all circumstances. Rather the ques-
tion should be: “Are there aspects of the way that the data were
                                                                                                                    regression with linear terms and factors 155

generated that might lead to some form of dependence?” Thus, when
data are collected over time, there may be a time series correlation
between points that are close together in time.

    Another possibility is some kind of clustering in the data, where
observations in the same cluster are correlated. In medical applica-
tions, it is common to have multiple observations on the one individ-
ual. Where clusters may be present, but there is no way to identify
them, dependence is hard or impossible to detect.

    Issues of dependence can arise in an engineering maintenance
context. If the same mechanic services two aircraft engines at the
same time using replacement parts from the same batch, this greatly
increases the chances that the same mistake will be made on the two
engines, or the same faulty part used. Maintenance faults are then
not independent. Independence is not the harmless assumption that it
is often made out to be!

    There may be further checks and tests that should be applied.
These may be speciﬁc to the particular model.

8.3 ∗Simulation Based Checks

A good way to check whether indications of departures from the                                                                                                  If the assumption of independent
model may be a result or random variation is to compare the plot                                                                                                random errors is wrong, patterns in
with similar plots for several sets of simulated data values, as a                                                                                              the diagnostic plots that call for an
means of verifying that the mismatch is, if residuals from the line                                                                                             explanation may be more common
are independent and normally distributed, real. This is the motiva-                                                                                             than suggested by the simulations.
tion for Figure 8.7.
                                                                                                                                                                Figure 8.7: The plots are four simu-
                                                 Simulated residuals q          Actual residuals q                                                              lations of residuals, from the model
                                                                                                                                                                that is ﬁtted to the unlogged data. The
                                                 1234                                                            1234                                           coeﬃcients used, and the standard
                                                                                                                                                                deviation, are from the ﬁtted least
                     Simulation1                        Simulation2             Simulation3                             Simulation4                             squares line.

            0.2  qq                              q                          qq         q                         q                                           q
            0.1                                                                                                                                              q
Residuals   0.0  qqqqqqqqqqqqqqqqqqqqqqqq  q     q qqqqqqqqqqqqqqqqqqqqq q              q                        q qqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqq qq q
           −0.1                            q  q         qqqqqqqqqqq q q     q qqqqqqqqqqqqqqqqqqqqqqqqqqqqq q q         q
           −0.2                                         qq                                                               q
                 qqqqqqqq                                 qq                                                                   q
                                                         q
                 qq                                            q                qq
                                                                                    q

                     1234                                                1234                                    q
                                q
                                                 qq

                                                   Time (h) for males

## Code
gph <- plotSimScat(obj=mftime.lm , show="residuals",

                                  type=c("p","smooth"),
                                  layout=c(4,1))
update(gph, xlab="Time (h) for males",
           ylab="Residuals")

    The simulations indicate that there can be a pattern in the smooth
curve that is largely due to the one point that is widely separated
from other data. On the other hand, the very large residual seen in
the actual data is not matched in any of the simulations.

    This type of check can be repeated for the other diagnostic plots:
156 learning and exploring r

- A check for normality (Panel 2: which=2. Type:

    plotSimDiags(obj=mftime.lm , which=2, layout=c(4,1))

- Is the variance constant? (Panel 3: which=3. Type:

    plotSimDiags(obj=mftime.lm , which=3, layout=c(4,1))

- Are there issues of leverage and inﬂuence? (Panel 4: which=5.
   Type:

    plotSimDiags(obj=mftime.lm , which=5, layout=c(4,1))

Scatterplots that are derived from the simulation process

To see scatterplots that are derived from the simulation process,
use the function plotsimscat(), from the DAAG package. For
example, try, working with the untransformed data:

 gph <- plotSimScat(mftime.lm , layout=c(4,1))
 update(gph, xlab="Male record times (h)",

             ylab="Female record times (h)")

Observe that the largest simulated value lies consistently above the
data value. Other simulated values, for male times of more than
around one hour, tend to lie below the actual data. This is much
easier to see in the plots of residuals. A scatterplot that shows the
actual data values is not a good tool for making these diﬀerence
visually obvious.

8.4 Key questions for the use of models

Key questions are:
• Modeling and analysis

  – Which model?
  – Do we want to make predictions? Or is the interest in getting

      parameter estimates that are interpretable?
  – How will model performance be measured?
  – How close can we get to measuring the performance that mat-

      ters?

• Interpretation

  – The task is easier if the aim is prediction, rather than interpreta-
      tion of model parameters.

  – Can model parameters be interpreted in scientiﬁcally meaning-
      ful ways?
      [This is a mineﬁeld, with huge scope for getting it wrong.]

    More detailed comments will now follow on some of the issues
raised above.
regression with linear terms and factors 157

The choice of method: Note the use of the word “method”, not
algorithm. Algorithms specify a sequence of computational steps.
Something more than an algorithm is needed, if results are to have
some use that generalizes beyond the speciﬁc data used.

    There are many diﬀerent methods. How should the analyst
choose between them? What are good ways to assess the perfor-
mance of one or other algorithm? A credible measure of model
performance is needed, evaluated on test data that closely reﬂects the
context in which the model will be applied.

Which are the important variables? Often, the analyst would like           5 Rosenbaum, P.R, 2002. Observa-
to know which data columns (variables, or features) were important         tional Studies, 2nd edn. Springer-
for, e.g., a classiﬁcation. Could some of them be omitted without          Verlag.
loss?

    The analyst may wish to attach an interpretation to one or more
coeﬃcients? Does the risk of heart attack increase with the amount
that a person smokes? For a meaningful interpretation of model
parameters, it is necessary to be sure that:

• All major variables or factors that aﬀect the outcome have been
   accounted for.

• Those variables and factors operate, at least to a ﬁrst order of
   approximation, independently.

    In some cases, a diﬀerent but equivalent choice of parameters
will be more meaningful. For working with the Northern Ireland
hillrace data in Subsection 8.6, the parameters dist and climb
clearly do not exercise their eﬀects independently, making their
coeﬃcients diﬃcult to interpret. It is better to work with log(dist)
and log(dist/climb), which are very nearly independent.

    See Rosenbaum’s Observational Studies5 for comments on ap-
proaches that are often useful in the attempt to give meaningful
interpretations to coeﬃcients that are derived from observational
data.

8.5 Factor Terms – Contrasts                                               Another special type of term is one
                                                                           that allows smooth functions of
Here, we show how regression models can be adapted to ﬁt terms             explanatory variables. Again, linear
involving factors.                                                         models can be adapted to handle such
                                                                           terms.
    Additive A is conc nutrient, B is 3x conc nutrient, and C
is 2-4-D + conc nutrient. For convenience, we label the factor
levels Water, A, B, and C, in that order.

lev <- c("Water", "A", "B", "C")
tomato[, "trt"] <- factor(rep(lev, rep(6,4)),

                                               levels=lev)

Taking Water as the initial level the eﬀect, in the ﬁrst analysis that is
given below, that it is treated as a reference level.
158 learning and exploring r

Water              A                B                                                      C             Table 8.1: Root weights (weight) (g)
(Water only)       (Additive 1)     (Additive 2)                                           (Additive 3)  of tomato plants, grown with water
                   1.50             1.90                                                   1.00          only and grown wth three diﬀerent
          1.50     1.20             1.60                                                   1.20          treatments. Data are in the data frame
          1.90     1.20             0.80                                                   1.30          tomato (DAAG 1.17 or later).
          1.30     2.10             1.15                                                   0.90
          1.50     2.90             0.90                                                   0.70          Figure 8.8: Termplot summary of
          2.40     1.60             1.60                                                   0.80          the one-way analysis of variance
          1.50     0.983            1.75                                                   0.983         result — A: for the analysis that uses
Mean = 1.683                                                                                             weights as the outcome variable, and
                                                                                                         B: for the analysis that works with
8.5.1 Example – tomato root weight                                                                       log(weight)

The model can be ﬁtted either using the function lm() or using the
function aov(). The two functions give diﬀerent default output. The
main part of the calculations is the same whether lm() or aov() is
used.

    For model terms that involve factor(s), there are several diﬀer-
ent ways to set up the relevant columns of the model matrix. The
default, for R and for many other computer programs, is to take one
of the treatment levels as a baseline or reference, with the eﬀects
of other treatment levels then measured from the baseline. Here it
makes sense to set Water as the baseline.

    Table 8.2 shows the model matrix when Water is taken as the
baseline. Values of the response (tomato$weight) have been added
in the ﬁnal column. Also included, in the column headers, is infor-
mation from the least squares ﬁt.

    The following uses aov() for the calculations:

## Analysis of variance: tomato data (from DAAG)
tomato.aov <- aov(weight ~ trt, data=tomato)

    Figure 8.8A is a useful summary of what the analysis has
achieved. The values are called partials because the overall mean
has been subtracted oﬀ. Figure 8.8B that is shown alongside shows
the eﬀect of working with the logarithms of weights. The scatter
about the mean for the treatment still appears much larger for the
controls than for other treatments.

   A: weight                                                                    B: log(weight)

1.5 q                                                                        0.8 q

Partial for treatment1.0 q                                                  0.6  q      q
                                                      Partial for treatment 0.4
                q                                                           0.2  q      q       q
                                                                            0.0         q       q
0.5 q              q                                                       −0.2  q
                                                                                 q      q       q
                qq                                                                                 q
                                                                                                   q
0.0 q           q
                                                                                                   q
      q                    q                                                                       q
                           q                                                                       q
                qq                                                         −0.4                    q
                           q                                               −0.6
−0.5               q       q                                                                    q  C
                   q       q                                                                    q
                           q
      Water     A  B                                                             Water  A       B
                           C

                Treatment                                                               Treatment

Code for Figures 8.8A and 8.8B is:
                   regression with linear terms and factors 159

## Panel A: Use weight as outcome variable
tomato.aov <- aov(weight ~ trt, data=tomato)
termplot(tomato.aov , xlab="Treatment",

                ylab="Partial for treatment",
                partial.resid=TRUE, se=TRUE, pch=16)
mtext(side=3, line=0.5, "A: weight", adj=0, cex=1.2)
## Panel B: Use log(weight) as outcome variable
logtomato.aov <- aov(log(weight) ~ trt, data=tomato)
termplot(logtomato.aov , xlab="Treatment",
                ylab="Partial for treatment",
                partial.resid=TRUE, se=TRUE, pch=16)
mtext(side=3, line=0.5, "B: log(weight)", adj=0,
           cex=1.2)

    Residuals, if required, can be obtained by subtracting the ﬁtted
values in Table 8.2 from the observed values (y) in Table 8.1.

    Coeﬃcient estimates for the model that uses weight as the de-
pendent variable, taken from the output summary from R, are:

round(coef(summary.lm(tomato.aov)),3)

      Estimate Std. Error t value Pr(>|t|)

(Intercept) 1.683  0.187 9.019 0.000

trtA  0.067        0.264 0.253 0.803

trtB  -0.358       0.264 -1.358 0.190

trtC  -0.700       0.264 -2.652 0.015

   The row labeled (Intercept) gives the estimate (= 1.683) for
the baseline, i.e., Water. The remaining coeﬃcients (diﬀerences
from the baseline) are:

  A: weight diﬀers by 0.067.

  B: weight diﬀers by −0.358.

  C: weight diﬀers by −0.700.

Regression calculations have given us a relatively complicated way
to calculate the treatment means! The methodology shows its power
to better eﬀect in more complex forms of model, where there is no
such simple alternative.

    Examination of the model matrix can settle any doubt about how
to interpret the coeﬃcient estimates, The ﬁrst four columns of Table
8.2 comprise the model matrix, given by:

model.matrix(tomato.aov)

The multipliers determined by least squares calculations are
shown above each column. Also shown is the ﬁtted value,
which can be calculated either as fitted(tomato.aov) or as
predict(tomato.aov).

8.5.2 Factor terms – diﬀerent choices of model matrix

In the language used in the R help pages, diﬀerent choices of con-
trasts are available, with each diﬀerent choice leading to a diﬀerent
160 learning and exploring r

Water: 1.683  A: +0.067       B: −0.358  C: −0.700  Fitted                Table 8.2: The model matrix for the
                                                    value                 analysis of variance calculation for
1             0               0          0          1.683                 the data in Table 8.1 is shown in gray.
1             0               0          0          1.683                 A fourth column has been added that
....                                                                      shows the ﬁtted values. At the head
1             0               0          0          1.683                 of each column is the multiple, as
1             1               0          0          1.750                 determined by least squares, that is
1             1               0          0          1.750                 taken in forming the ﬁtted values.
....
1             1               0          0          1.750                 Be sure to choose the contrasts that
1             0               1          0          1.325                 give the output that will be most
1             0               1          0          1.325                 helpful for the problem in hand. Or,
....                                                                      more than one run of the analysis
1             0               1          0          1.325                 may be necessary, in order to gain
1             0               0          1          0.983                 information on all eﬀects that are of
1             0               0          1          0.983                 interest.
....
1             0               0          1          0.983

model matrix and to diﬀerent regression parameters. The ﬁtted val-
ues remain the same, the termplot in Figure ﬁg:tomatotermA is
unchanged, and the analysis of variance table is unchanged.

    Where there is just one factor, the constant term can be omitted,
i.e., it is eﬀectively forced to equal zero. The parameters are then the
estimated treatment means. Specify:

## Omit constant term from fit;
## force parameters to estimate treatment means
tomatoM.aov <- aov(weight ~ 0 + trt, data=tomato)

    The ﬁrst nine rows of the model matrix are:

mmat <- model.matrix(tomatoM.aov)
mmat[1:9, ]

   trtWater trtA trtB trtC
1 1000
2 1000
3 1000
4 1000
5 1000
6 1000
7 0100
8 0100
9 0100

## ... ... ... ...

Observe that there is now not an initial column of ones. This is ﬁne
when there is just one factor, but does not generalize to handle more
than one factor and/or factor interaction.

    The default (treatment) choice of contrasts uses the initial factor
level as baseline, as we have noted. Diﬀerent choices of the baseline
                                                                             regression with linear terms and factors 161

or reference level lead to diﬀerent versions of the model matrix.
The other common choice, i.e., sum contrasts, uses the average of
treatment eﬀects as the baseline.

The sum contrasts

Here is the output when the baseline is the average of the treatment
eﬀects, i.e., from using the sum contrasts:

oldoptions <- options(contrasts=c("contr.sum",
                                                             "contr.poly"))

tomatoS.aov <- aov(weight ~ trt, data=tomato)
round(coef(summary.lm(tomatoS.aov)),3)

      Estimate Std. Error t value Pr(>|t|)

(Intercept) 1.435           0.093 15.381 0.000

trt1        0.248           0.162 1.534 0.141

trt2        0.315           0.162 1.946 0.066

trt3        -0.110          0.162 -0.683 0.502

options(oldoptions) # Restore default contrasts                              The estimates (means) are:
                                                                               Water: 1.435 + 0.248 = 1.683.
The baseline, labeled (Intercept), is now the treatment mean.                  A: 1.435 + 0.315 = 1.750.
This equals 1.435. Remaining coeﬃcients are diﬀerences, for Water              B: 1.435 − 0.110 = 1.325.
and for treatment levels A and B, from this mean. The sum of the               C: 1.435 − 0.452 = 0.983.
diﬀerences for all three treatments is zero. Thus the diﬀerence for C
is (rounding up)

               −(0.2479 + 0.3146 − 0.1104) = −0.4521.

    Yet other choices of contrasts are possible; see
help(contrasts).

Interaction terms

The data frame cuckoos has the lengths and breadths of cuckoo
eggs that were laid in the nexts of one of six diﬀerent bird species.
The following compares a model where the regression line of
breadth against length is the same for all species, with a model that
ﬁts a diﬀerent line for each diﬀerent cuckoo species:

cuckoos.lm <- lm(breadth ~ species + length , data=cuckoos)
cuckoosI.lm <- lm(breadth ~ species + length + species:length , data=cuckoos)
print(anova(cuckoos.lm , cuckoosI.lm), digits=3)

Analysis of Variance Table

Model 1: breadth ∼ species + length

Model 2: breadth ∼ species + length + species:length

Res.Df RSS Df Sum of Sq F Pr(>F)

1 113 18.4

2 108 17.2 5        1.24 1.56 0.18

Here, the model cuckoos.lm, where the regression lines are parallel
(the same slope for each species), appears adequate.

    An alternative way to compare the two models is:
162 learning and exploring r

anova(cuckoos.lm , cuckoosI.lm , test="Cp")

The Cp statistic (smaller is better) compares models on the basis of
an assessment of their predictive power. Note the use of the argu-
ment test="cp", even though this is not a comparison that is based
on a signiﬁcance text.

8.6 Regression with two explanatory variables

Data exploration

The dataset nihills in the DAAG package has record times for
Northern Ireland mountain races. First, get a few details of the data:

str(nihills)

'data.frame ':                                23 obs. of 4 variables:
 $ dist : num                                7.5 4.2 5.9 6.8 5 4.8 4.3 3 2.5 12 ...
 $ climb: int                                1740 1110 1210 3300 1200 950 1600 1500 1500 5080 ...
 $ time : num                                0.858 0.467 0.703 1.039 0.541 ...
 $ timef: num                                1.064 0.623 0.887 1.214 0.637 ...

    The following Figure 8.9 repeats Figure 3.6 from Chapter 3.5.
The left panel shows the unlogged data, while the right panel shows
the logged data:

                          q                             q                     q6                                                           qqq
                                                                                   5
                                                                           q                             456                                                                                       1.5 0.5 1.5
                                                                       q
                                                            qqqqqqqqqqq           4                                                     q                                 q                              q       1.0
                                                                                                                                       q                           q                                 q
                                                        q4                            timef 3                                                                                                                   0.5 ltimef
                       q                          q          3                                                 q qqqqqqqqqqqqqqqqq                qqqqqqqqqqqqqqq  qq                qqqqqqqqqqqqqqqq
                     q                       q                                                           2       q                             q                   q             q                                             0.0
                                                                                                                                           q                                 q
qqqqqqqqqqqqqqqqq            qqqqqqqqqqqqqq  qq                                   123                    1                                                                                                  −1.0 0.0 −0.5
                                             q                                                                                                                                                                                −1.0

                          q                                                                                 q                                                                                                                                    q

                                                                              34                                                                                                   1.0 0.00.51.0

                                                                                                                                     q                                    q        0.5                                                   q
                                                                                                                                    q                              q                                                                   q
                                                                                                                                                                                   0.0 ltime 0.0
                       q                          q           time            2q                                  q             q                     q            qq                                                          qqq
                     q                                                                                                           q            qqqqqqqqqqqqqq       q                            −0.5                    qqqqqqqqqqqqq
                                             q    6000     12                                   q              q qqqqqqqqqqqqqqq           q                                    −1.0 0.0 −1.0                       q

qqqqqqqqqqqqqqqqqqq          qqqqqqqqqqqqqq  qq                               1 qqqqqqqqqqq
                                             q
                                                                                  q
                                                                                                               q
                                                                                                   q
                          q                                                              qqq q              q                           q 9.0                                                                    q                                  q
                                                                                      qqqqqqqqqqq           q                                          8.0 8.5 9.0                                      q                                 q
                               8000                                               q
                                                                                                                                    q 8.5
                              6000                                                              qq
                     q                                                    q           qqqqqqqqqqqqqqqqq                        q    q      8.0 lclimb8.0                            q         q      q                        q        q
                                  climb4000                                                                           qq                                                                     q                          qq
                                                              qqq q                                                                                      7.5
   q qq              q       2000 2000                     qqqqqqqqqqqqq                                       q qqqqqqqqqqqqq q                                                   qqqqqqqqqqqqqqqq                 qqqqqqqqqqqqqqqq
qqqqqqqqqqqqq q                                                                                                  q                         7.07.58.0 7.0                        q                               q
                                                                                                                                                                                                            q
                                                        q                                                        3.0                                                         q                                                              q

      10 15                                                                                                    2.5 2.0 2.5 3.0                                     qq                                qq                                qq
15

10 dist 10                                   qq                      qq                                          2.0 ldist 2.0                        q              q                       qq                         qq
                                                           qqqqqqqqqqqqqqqqq                                                                     qq                q
5 10 5                           q                                                                                              1.5           qqqqqqqqq                               q      q                           qq      q
                             qqqqqqqqqqqqqq                                                                    1.0 1.5 2.0                                         q            qqqqqqqqqqq                         qqqqqqqqqqq
                                               q                                                                                                      q                         q                                   q
                                             q                                                                                  1.0        qq                                   qq                                  qq
                                             q
                                                                                                                                                    q
                                                                                                                                                                                q                                   q

                                                  Scatter Plot Matrix                                                                                              Scatter Plot Matrix

    The relationships between explanatory variables, and between                                                                                                   Figure 8.9: Scatterplot matrices for
the dependent variable and explanatory variables, are closer to linear                                                                                             the Northern Ireland mountain racing
when logarithmic scales are used. Just as importantly, the point with                                                                                              data. In the right panel, code has been
the largest unlogged values (the same for all variables) will have a                                                                                               added that shows the correlations.
leverage, inﬂuencing the ﬁtted regression, that is enormously larger                                                                                               This repeats Figure 3.6 from Chapter
than that of other points.                                                                                                                                         3.5.
                                                                             regression with linear terms and factors 163

    The log transformed data are consistent with a form of parsimony
that is well-designed to lead to a simple form of model. We will see
that this also leads to more readily interpretable results. Also the
distributions for individual variables are more symmetric.

    Here again is the code:

## Unlogged data
library(lattice)
## Scatterplot matrix; unlogged data
splom(~nihills)

    The right panel requires a data frame that has the logged data

## Logged data
lognihills <- log(nihills)
names(lognihills) <- paste0("l", names(nihills))
## Scatterplot matrix; log scales
splom(~ lognihills)

8.6.1 The regression ﬁt

The following regression ﬁt uses logarithmic scales for all variables:

lognihills <- log(nihills)
lognam <- paste0("l", names(nihills))
names(lognihills) <- lognam
lognihills.lm <- lm(ltime ~ ldist + lclimb ,

                                    data=lognihills)
round(coef(lognihills.lm),3)

(Intercept)      ldist   lclimb
         -4.961  0.681    0.466

Thus for constant climb, the prediction is that time per mile will      The ﬁtted equation gives predicted
decrease with increasing distance. Shorter races with the same climb    times:
will involve steeper ascents and descents.
                                                                                 e3.205 × dist0.686 × climb0.502
    A result that is easier to interpret can be obtained by regressing     = 24.7e3.205 × dist0.686 × climb0.502
log(time) on log(dist) and log(gradient), where gradient
is dist/climb.

nihills$gradient <- with(nihills , climb/dist)
lognihills <- log(nihills)
lognam <- paste0("l", names(nihills))
names(lognihills) <- lognam
lognigrad.lm <- lm(ltime ~ ldist + lgradient ,

                                  data=lognihills)
round(coef(lognigrad.lm),3)

(Intercept)      ldist lgradient
         -4.961
                 1.147   0.466

Thus, with gradient held constant, the prediction is that time will
increase at the rate of dist1.147. This makes good intuitive sense.

    We pause to look more closely at the model that has been ﬁtted.
Does log(time) really depend linearly on the terms ldist and
log(lclimb)? The function termplot() gives a good graphical
indication (Figure 8.10).
164 learning and exploring r

                                                                                q  1.5                                                 Figure 8.10: The vertical scales
                                                                                                                                       in both “term plot” panel show
1.5                                                                                                                                    log(time), centered to a mean of
                                                                                                                                       zero. The partial residuals in the left
1.0Partial for ldist                q                                              1.0                                                 panel are for ldist, while those in the
                                                      Partial for lgradientq                                                           right panel are for lgradient, i.e.,
                                                                                                                                       log(climb/dist). Smooth curves
 0.5                          q                                                    0.5                                        q        (dashes) have been passed through the
 0.0                                                                                                                                q  points.
−0.5                    q  q                                                                                            q qq
                      qq
                                                                                                          qq     q      qq
                qqqq                                                               0.0                    q          q
              qqqqq                                                                              qqqqqqq
                                                                                              q               q

                                                                                            q

      q qq q                                                                       −0.5

      1.0 1.5 2.0 2.5                  3.0                                                       5.4 5.6 5.8 6.0 6.2 6.4

                   ldist                                                                                  lgradient

## Plot the terms in the model
termplot(lognigrad.lm , col.term="gray", partial=TRUE,

                col.res="black", smooth=panel.smooth)

    The vertical scales show changes in ltime, about the mean of
ltime. The lines show the estimated eﬀect of each explanatory
variable when the other variable is held at its mean value. The lines,
which are the contributions of the individual linear terms (“eﬀects”)
in this model, are shown in gray so that they do not obtrude unduly.
The dashed curves, which are smooth curves that are passed through
the residuals, are the primary features of interest.

    Notice that, in the plot for ldist, the smooth dashed line does
not quite track the ﬁtted line; there is a small but noticeable in-
dication of curvature that can be very adequately modeled with a
quadratic curve. Note also that until we have modeled eﬀectively the
clear trend that seems evident in this plot, there is not much point in
worrying about possible outliers.

  8.7 Variable Selection – Stepwise and Other                                                                                          The points made here can have highly
                                                                                                                                       damaging implications for analyses
   Common variable selection methods include various versions of for-                                                                  where it is important to obtain inter-
   ward and backward selection, and exhaustive (best subset) selection.                                                                pretable regression coeﬃcients. In
   These or other variable selection methods invalidate standard model                                                                 such analyses, changes to the initial
   assumptions, which assume a single known model.                                                                                     model should be limited to simpliﬁca-
                                                                                                                                       tions that do not modify the model in
       There are (at least) three inter-related issues for the use of results                                                          any substantial manner. Following the
   from a variable selection process:                                                                                                  selection process, check coeﬃcients
                                                                                                                                       against those from the full model. Any
(i) Use of standard theoretically based model ﬁtting procedures,                                                                       large changes should ring a warning
     applied to the model that results from the model selection process,                                                               bell.
     will lead to a spuriously small error variance, and to a spuriously
     large model F-statistic. Coeﬃcient estimates will be inﬂated, have                                                                  The implications for prediction are,
     spuriously small standard errors, and spuriously large t-statistics.                                                              relatively, much more manageable.
     (Or to put the point another way, it is inappropriate to refer such                                                               The key requirement is to use indepen-
     statistics to a standard t-distribution.)                                                                                         dent data to show that any selective
                                                                                                                                       process has genuinely improved model
(ii) Commonly used stepwise and other model selection processes are                                                                    performance.
     likely to over-ﬁt, i.e., the model will not be optimal for prediction
     on test data that are distinct from the data used to train the model.
                                                                                 regression with linear terms and factors 165

      The selected model may in some instances be inferior, judged
      by this standard, to a model that uses all candidate explanatory
      variables. (There are alternative ways to use all variables. Should
      low order interactions be included? Should some variables be
      transformed?)

(iii) Coeﬃcients may change, even to changing in sign, depending on
      what else is included in the model. With a diﬀerent total set of
      coeﬃcients, one has a diﬀerent model, and the coeﬃcients that
      are common across the two models may be accordingly diﬀerent.
      (They will be exactly the same only in the unusual case where
      “variables” are uncorrelated.) There is a risk that variable selec-
      tion will remove variables on whose values (individually, or in
      total eﬀect) other coeﬃcient estimates should be conditioned. This
      adds uncertainty beyond what arises from sampling variation.

       Note that these points apply to pretty much any type of regres-
   sion modelling, including generalized linear models and classiﬁca-
   tion (discriminant) models.

       Where observations are independent, items (i) and (ii) can be
   addressed, for any given selection process, by splitting the data into
   training, validation and test sets. Training data select the model,
   with the validation data used to tune the selection process. Model
   performance is then checked against the test data.

       Somewhat casual approaches to the use of backward (or other)
   stepwise selection may be a holdover from hand calculator days,
   or from times when computers grunted somewhat to handle even
   modest sized calculations. This may be one of the murky dark alleys
   of statistical practice, where magic incantations and hope too often
   prevail over hard evidence.

       Appropriate forms of variable selection process can however be
   eﬀective in cases where a few only of the coeﬃcients have predic-
   tive power, and the relevant t-statistics are large – too large to be
   substantially inﬂated by selection eﬀects.

8.7.1 Use of simulation to check out selection eﬀects:                 6 See also Section 6.5, pp. 197-198, in:
                                                                       Maindonald, JH and Braun, WJ, 2010.
The function bestsetNoise() (DAAG) can be used to experiment           Data Analysis and Graphics Using R
with the behaviour of various variable selection techniques with data  – An Example-Based Approach, 3rd
that is purely noise. For example, try:6                               edition. Cambridge University Press.

bestsetNoise(m=100, n=40, nvmax=3)
bestsetNoise(m=100, n=40, method="backward",

                       nvmax=3)

The analyses will typically yield a model that appears to have highly
(but spuriously) statistically signiﬁcant explanatory power, with one
or more coeﬃcients that appear (again spuriously) signiﬁcant at a
level of around p=0.01 or less.
  166 learning and exploring r                                                                       0.75        Select 'best' 3 variables

 The extent of selection eﬀects – a detailed simulation: As above,                                               q
 datasets of random normal data were created, always with 100 obser-
 vations and with the number of variables varying between 3 and 50.       p−values for t−statistics              qq
 For three variables, there was no selection, while in other cases the
 “best” three variables were selected, by exhaustive search.                                            0.5  qq

     Figure 8.11 plots the p-values for the 3 variables that were se-                                 0.25   qq q  q
 lected against the total number of variables. The ﬁtted line estimates                                        q      qq
 the median p-value. Code is:                                                                         0.05    q
                                                                                                      0.01    qq   qqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqq
  library(DAAG)                                                                                      0.001     q
  library(quantreg)
  library(splines)                                                                                                   q
  set.seed(37) # Use to reproduce graph shown
  bsnVaryNvar(m=100, nvar=3:50, nvmax=3, fg="gray")                                                   0 10 20 30 40 50

     When all 3 variables are taken, the p-values are expected to                                    # of variables from which to select
 average 0.5. Notice that, for selection of the best 3 variables out
 of 10, the median p-value has reduced to about 0.1.                      Figure 8.11: p-values, versus number
                                                                          of variables available for selection,
 Examples from the literature The paper cited in the sidenote7 gives      when the “best” 3 variables were
 several examples of published spurious results, all for the use of       selected by exhaustive search. The
 discriminant methods with microarray data. The same eﬀects can           ﬁtted line estimates the median p-
 arise from model tuning.                                                 value.

 8.7.2 Variable and model selection – strategies                          7 Ambroise, C and McLachlan, GJ,
                                                                          2001. Selection bias in gene extraction
 Several alternative mechanisms are available that can yield reason-      on the basis of microarray gene-
 able standard errors and other accuracy measures. These include:         expression data. Proceedings of the
                                                                          National Academy of Sciences USA,
a) Fit the model to test data that have played no part in the model       99: 6562-6566.
    selection and tuning process;
                                                                          If however the coeﬃcients are not
b) use cross-validation. The model selection and ﬁtting process must      themselves very meaningful, what is
    be repeated at each cross-validation fold;                            the point?

c) repeat the whole analysis, selection and all, with repeated boot-
    strap samples, using variation between the diﬀerent sample results
    to assess the accuracy of one or other statistic;

d) simulate, including all selection and tuning steps, from the ﬁtted
    model.

 For b) and c), there will be somewhat diﬀerent selections for each
 diﬀerent cross-validation fold or bootstrap sample. This is itself
 instructive.

     One possibility, following stepwise or other selection, is that the
  p-values of one or more coeﬃcients may be so small that they are
 very unlikely to be an artefact of the selection process. In general, a
 simulation will be required, in order to be sure.

Model selection more generally:                                           Use of test data that are separate from
                                                                          data used to develop the model deals
More generally, the model may be chosen from a wide class of              with this issue.
                                                                             regression with linear terms and factors 167

models. Again, model selection biases standard errors to be smaller
than indicated by the theory, and coeﬃcients and t-statistics larger.
The resulting anti-conservative estimates of standard errors and other
statistics should be regarded sceptically.

    A further issue, which use of separate test data does not address,
is that none of the models on oﬀer is likely to be strictly correct.
Mis-speciﬁcation of the ﬁxed eﬀects will bias model estimates, at the
same time inﬂating the error variance or variances. Thus it will to an
extent work in the opposite direction to selection eﬀects.

8.8 1970 cost for US electricity producers

There is a wide range of possible choices of model terms. Figure
8.12 shows the scatterplot matrices of the variables. Code is:

library(car)
library(Ecdat)
data(Electricity)
spm(Electricity , smooth=TRUE, regLine=FALSE,

       col=adjustcolor(rep("black",3), alpha.f=0.3))

                      0 40000 100000              0.05 0.15 0.25               0.1 0.2 0.3 0.4                  0.3 0.5 0.7

           cost                                                                                                                                 600
                                                                                                                                                400
                                                                                                                                                200
                                                                                                                                                0

120000                q
100000                              pl

 80000
 60000
 40000
 20000

        0

                                                                                                                12000

                                                                                                                10000 cost: total cost

                                                                                                                8000
                                                                                                                6000

                                                                                                                            q: total output

0.30                                              sl                                                                                                           pl: wage rate
0.25                                                            pk
0.20                                                                          sk                                                                               sl: cost share, labor
0.15
0.10
0.05

                                                                                                                90                                             pk: capital price index
                                                                                                                80

                                                                                                                70

                                                                                                                60

                                                                                                                50 sk: cost share, capital
                                                                                                                40

                                                                                                                30

                                                                                                                                                               pf: fuel price

0.4                                                                                                                                                            sf: cost share, fuel
0.3
0.2                                                                                             pf 50
0.1                                                                                                                                                        40
                                                                                                                                                           30
0.8                                                                                                                                                        20
0.7                                                                                                                                                        10
0.6
0.5                                                                                                           sf
0.4
0.3                                   6000 10000                  30 50 70 90                   10 20 30 40 50

       0 200 400 600

                                                                                                                                                               Figure 8.12: Scatterplot matrix,
                                                                                                                                                               for the variables in the data set
                                                                                                                                                               Electricity, in the Ecdat pack-
                                                                                                                                                               age. Density plots are shown in the
                                                                                                                                                               diagonal.
    168 learning and exploring r                                              Removal of terms with p > 0.15 or
                                                                              p > 0.2 rather than p > 0.05 greatly
   8.8.1 Model ﬁtting strategy                                                reduces the risk that estimates of other
                                                                              parameters, and their standard errors,
   The analysis will start by checking for clearly desirable transfor-        will change in ways that aﬀect the
   mations to variables. Then, for obtaining a model whose whose              interpretation of model results.
   parameters are as far as possible interpretable, a strategy is:
                                                                                        log(cost)  2 4 6 8 10 12
 (i) Start with a model that includes all plausible main eﬀects (vari-                                                                                             6
      ables and factors). Ensure that the model is parameterised in a         12                                                                                   4
      way that makes parameters of interest as far as possible inter-         10                                                                                   2
      pretable (e.g., in Subsection 3.5 above, work with distance and                                                                                              0
      gradient, not distance and climb)                                        8                                                                                   −2
                                                                               6
(ii) [21pt] Model simpliﬁcation may be acceptable, if it does not              4                          log(q)
      change the parameters of interest to an extent that aﬀects inter-        2
      pretation. The common p > 0.05 is too severe; try instead p =
      0.15 (remove terms with p > 0.15) or p = 0.20.                                −2 0 2 4 6

(iii) Variables and/or factors that have no detectable main eﬀect are in      Figure 8.13: Scatterplot matrix for
      general unlikely to show up in interactions. Limiting attention to      the logarithms of the variables cost
      the main eﬀects that were identiﬁed in (ii) above, we then compare      and q. Density plots are shown in the
      a model which has only main eﬀects with a model that inludes all        diagonal.
      2-way interactions. Then, using p 0.15 or p 0.2 as the cutoﬀ,
      remove interaction terms that seem unimportant, and check that
      there are no changes of consequence in terms that remain.

(iv) In principle, the process may be repeated for order 3 interactions.

 (v) Use the function add1() to check for individual highly signiﬁcant
      terms that should be included. For this purpose, we might set
      p = 0.01 or perhaps p = 0.001.

   The strategy is to be cautious (hence the cutoﬀ of p = 0.2) in re-
   moving terms, whether main eﬀects or ﬁrst order interactions. In a
   ﬁnal check whether there is a case for adding in terms that had been
   omitted, we include a term only if it is highly statistically signiﬁcant.
   This limits the scope for selection eﬀects.

   Distributions of variables

   The distributions of cost and q are highly skew. The relationship
   between these two variables is also very close to linear. We might try
   taking logarithms of both these variables.

       Figure 8.13 examines the scatterplot matrix for the logarithms of
   the variables cost and q. Code is:

    varlabs <- c("log(cost)", "log(q)")
    spm(log(Electricity[,1:2]), var.labels=varlabs,

            smooth=TRUE, regLine=FALSE,
            col=adjustcolor(rep("black",3), alpha.f=0.5))

   We start with a model that has main eﬀects only:

    elec.lm <- lm(log(cost) ~ log(q)+pl+sl+pk+sk+pf+sf,
                              data=Electricity)
                                                                                                                                                                                                                                                                                                                                                                                           regression with linear terms and factors 169

Partial for log(q)             qqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqq                q                                                                                                                                                                                                                                                                                                                                                                Figure 8.14: Termplot summary for
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       the model that has been ﬁtted to the
                     2                                                                                                   Partial for pl   2                                                                                                qq   Partial for sl   2                                                                                                     Partial for pk   2                                                                                              Electricity dataset.
                     0                                                                                                                    0 qqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqq                        0 qqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqq qqqqqqqqqqq q                   0 qqqqqqqqq qqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqq
                    −2
                    −4                                                                                                                   −2                                                                                                                     −2                                                                                                                     −2

                        q qqq                                                                                                            −4                                                                                                                     −4                                                                                                                     −4

                    −6                                                                                                                   −6                                                                                                                     −6                                                                                                                     −6
                                                                                                                                                6000                                                                                                               0.05                                                                                                                   30
                        −6 −4 −2 0 2                                                                                                                   10000                                                                                                                0.15 0.25                                                                                                         50 70  90

                               log(q)                                                                                                                 pl                                                                                                                       sl                                                                                                                pk

Partial for sk      2                                                                                                    Partial for pf   2                                                                                                     Partial for sf   2

                    0   qq  qqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqq  qq      q  q                   0        qqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqq qq                         qq   qq qqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqq qq
                                                                                                               qq
                                                                                                                q                               q                                                                                                                0

                    −2                                                                                                                   −2                                                                                                                     −2

                    −4                                                                                                                   −4                                                                                                                     −4

                    −6         0.2 0.3                                                                         0.4                       −6                                                                                                                     −6          0.5 0.7
                        0.1                                                                                                                                                                                                                                            0.3
                                   sk                                                                                                         10 20 30 40 50                                                                                                                 sf

                                                                                                                                                      pf

    Now examine the termplot (Figure 8.14): Code is:

termplot(elec.lm, partial=T, smooth=panel.smooth ,
                transform.x=TRUE)

Notice that in the partial plot for q, the dashed curve that is ﬁtted
to the residuals closely tracks the ﬁtted eﬀect (linear on a scale of
log(q). This conﬁrms the use of log(q), rather than q, as explana-
tory variable.

    Now examine the model output:

round(coef(summary(elec.lm)),5)

                                                                                                               Estimate Std. Error t value Pr(>|t|)

(Intercept) -5.41328 0.70720 -7.6545 0.00000

log(q)                                                                                                                0.89250 0.00994 89.8326 0.00000

pl -0.00002 0.00001 -1.9341 0.05499

sl 2.48020 0.74898 3.3114 0.00116

pk 0.00083 0.00127 0.6562 0.51272

sk 0.62272 0.70837 0.8791 0.38076

pf 0.03042 0.00228 13.3338 0.00000

sf -0.30965 0.69091 -0.4482 0.65467

    The p-values suggest that pk, sk, and sf can be dropped from
the model. Omission of these terms makes only minor diﬀerences to
the coeﬃcients of terms that remain.

elec2.lm <- lm(log(cost) ~ log(q)+pl+sl+pf,
                           data=Electricity)

round(coef(summary(elec2.lm)),5)

                                                                                                               Estimate Std. Error t value Pr(>|t|)

(Intercept) -5.28641 0.13701 -38.585 0.00000

log(q)                                                                                                                0.88901 0.00986 90.167 0.00000

pl -0.00002 0.00001 -2.072 0.03994

sl 2.69722 0.32464 8.308 0.00000

pf 0.02659 0.00191 13.934 0.00000

   Now check whether interaction terms should be included:

elec2x.lm <- lm(log(cost) ~ (log(q)+pl+sl+pf)^2,
                             data=Electricity)

anova(elec2.lm, elec2x.lm)

Analysis of Variance Table
170 learning and exploring r

Model 1: log(cost) ∼ log(q) + pl + sl + pf
Model 2: log(cost) ∼ (log(q) + pl + sl + pf)∧2

    Res.Df RSS Df Sum of Sq F Pr(>F)

1 153 5.00

2 147 2.81 6           2.19 19.1 2.3e-16

The case for including ﬁrst order interactions seems strong. The
coeﬃcients and SEs are:

round(coef(summary(elec2x.lm)),5)

         Estimate Std. Error t value Pr(>|t|)

(Intercept) -3.63931 0.67803 -5.3675 0.00000

log(q)      0.71481 0.05455 13.1039 0.00000

pl -0.00031 0.00007 -4.2508 0.00004

sl 6.06389 1.64592 3.6842 0.00032

pf 0.01592 0.01499 1.0623 0.28985

log(q):pl 0.00003 0.00001 6.2902 0.00000

log(q):sl -0.67829 0.10229 -6.6308 0.00000

log(q):pf 0.00080 0.00113 0.7133 0.47680

pl:sl       0.00007 0.00018 0.4144 0.67916

pl:pf       0.00000 0.00000 0.1421 0.88722

sl:pf       0.01680 0.03092 0.5432 0.58780

This suggests omitting the terms pf, and all interactions except
log(q):pl and log(q):sl. We check that omission of these terms
makes little diﬀerence to the terms that remain:

elec2xx.lm <- lm(log(cost) ~ log(q)+pl+sl+pf+
                               log (q ): pl + log (q ): sl ,
                               data=Electricity)

round(coef(summary(elec2xx.lm)),5)

         Estimate Std. Error t value Pr(>|t|)

(Intercept) -4.12003 0.33312 -12.368            0

log(q)      0.74902 0.03852 19.445              0

pl       -0.00029 0.00004 -7.755                0

sl          7.29642 0.70758 10.312              0

pf          0.02611 0.00145 18.017              0

log(q):pl 0.00003 0.00000 7.657                 0

log(q):sl -0.68969 0.09435 -7.310               0

    Now check whether there is a strong case for adding in any fur-
ther individual terms:

add1(elec2xx.lm , scope=~(log(q)+pl+sl+pk+sk+pf+sf)^2, test="F")

Single term additions

Model:

log(cost) ∼ log(q) + pl + sl + pf + log(q):pl + log(q):sl

         Df Sum of Sq RSS AIC F value Pr(>F)

<none >                2.83 -622

pk 1 0.0041 2.82 -620 0.22 0.64

sk 1 0.0329 2.79 -621 1.76 0.19

sf 1 0.0294 2.80 -621 1.58 0.21

log(q):pf 1 0.0040 2.82 -620 0.21 0.64

pl:sl    1 0.0060 2.82 -620 0.32 0.57
pl:pf                                                                        regression with linear terms and factors 171
sl:pf
                  1 0.0004 2.83 -620 0.02 0.88
                  1 0.0016 2.83 -620 0.09 0.77

8.9 An introduction to logistic regression                           The dataset bronchit may alterna-
                                                                     tively be found in the SMIR package.
The data that will be used for illustration are from the data frame
bronchit in the DAAGviz package. The following loads packages
that will be needed:

library(DAAGviz, quietly=TRUE)
library(KernSmooth , quietly=TRUE)

    Figure 8.15 shows two plots – one of poll (pollution level)
against cig (number of cigarettes per day), and the other of poll
against log(poll). In each case, points are identiﬁed as with or
without bronchitis.

                  70 Non−sufferer Sufferer               70                        Non−sufferer          Sufferer

       Pollution  65                                     65 0.01                                   0.02
                                                                                                 0.03
                  60                                                     0.02

                  55                                     60

                        0 5 10 15 20 25 30                                   0.03

                               # cigarettes per day      55

                                                                                                                           0.01

                                                                     0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5

                                                                         log(# cigarettes per day + 1)

## Panel A                                                           Figure 8.15: Panel A plots poll
colr <- adjustcolor(c("red","blue"), alpha=0.5)                      (pollution level) against cig (number
plot(poll ~ cig,                                                     of cigarettes per day). In panel B, the
                                                                     x-scale shows the logarithm of the
         xlab="# cigarettes per day", ylab="Pollution",              number of cigarettes per day.
         col=colr[r+1], pch=(3:2)[r+1], data=bronchit ,
         ylim=ylim)
legend(x="topright",

            legend=c("Non-sufferer","Sufferer"),
            ncol=2, pch=c(3,2), col=c(2,4), cex=0.8)

## Panel B
plot(poll ~ log(cig+1), col=c(2,4)[r+1], pch=(3:2)[r+1],

         xlab="log(# cigarettes per day + 1)", ylab="", data=bronchit , ylim=ylim)
xy1 <- with(subset(bronchit , r==0), cbind(x=log(cig+1), y=poll))
xy2 <- with(subset(bronchit , r==1), cbind(x=log(cig+1), y=poll))
est1 <- bkde2D(xy1, bandwidth=c(0.7, 3))
est2 <- bkde2D(xy2, bandwidth=c(0.7, 3))
lev <- pretty(c(est1$fhat, est2$fhat),4)
172 learning and exploring r

contour(est1$x1, est1$x2, est1$fhat, levels=lev, add=TRUE, col=2)
contour(est2$x1, est2$x2, est2$fhat, levels=lev, add=TRUE, col=4, lty=2)
legend(x="topright", legend=c("Non-sufferer","Sufferer"), ncol=2, lty=1:2,

    The logarithmic transformation spreads the points out in the x-
direction, in a manner that is much more helpful for prediction than
the untransformed values in panel A. The contours for non-suﬀerer
and suﬀerer in panel B have a similar shape. The separation between
non-suﬀerer and suﬀerer is stronger in the x-direction than in the y-
direction. As one indication of this, the contours at a density of 0.02
overlap slightly in the x-direction, but strongly in the y-direction.

Logistic regression calculations

Figure 8.15 made it clear that the distribution of number of cigarettes
had a strong positive skew. Thus, we might ﬁt the model:

cig2.glm <- glm(r ~ log(cig+1) + poll, family=binomial , data=bronchit)
summary(cig2.glm)

Call:
glm(formula = r ∼ log(cig + 1) + poll, family = binomial , data = bronchit)

Deviance Residuals:                Max
     Min 1Q Median 3Q           2.653

-1.611 -0.586 -0.362 -0.239

Coefficients:

               Estimate Std. Error z value Pr(>|z|)

(Intercept) -10.7877   2.9885 -3.61 0.00031

log(cig + 1) 1.2882    0.2208 5.83 5.4e-09

poll           0.1306  0.0494 2.64 0.00817

(Dispersion parameter for binomial family taken to be 1)

       Null deviance: 221.78 on 211 degrees of freedom
Residual deviance: 168.76 on 209 degrees of freedom
AIC: 174.8

Number of Fisher Scoring iterations: 5

       3                                                                       3                        Figure 8.16: The panels show the
                                                                                                        contributions that the respective terms
       2                                                                       2                        make to the ﬁtted values (logit of
                                                                                                        probability of bronchitis), when the
       1                                                                       1                        other term is held constant.

       0                                                                       0

      −1                                                                      −1
            0 5 10 15 20 25 30                                                              55 60 65

                          cig                                                                     poll
      Partial for log(cig + 1)
                                                            Partial for poll
                                                                             regression with linear terms and factors 173

    Termplots (Figure 8.16) provide a useful summary of the contri-
butions of the covariates. For binary (0/1) data such as here, includ-
ing the data values provides no visually useful information. Code
is:

termplot(cig2.glm)

8.10 Regression with Fitted Smooth Curves

Load the DAAG package:

Commentary on Smoothing Methods                                          Smoothness is controlled by the
                                                                         width of the smoothing window. The
Two types of methods will be described – those where the user con-       default is f=2/3 for lowess(), or
trols the choice of smoothing parameter, and statistical learning type   span=0.75 for loess(). For other
methods where the amount of smoothing is chosen automatically:           functions that rely on this methodol-
                                                                         ogy, check the relevant help page.
• The ﬁrst class of methods rely on the user to make a suitable
   choice of a parameter that controls the smoothness. The default       8 Strong assumptions are required,
   choice is often a good ﬁrst approximation. Note here:                 notably that observations are inde-
                                                                         pendent. Normality assumptions are,
  – Smoothing using a “locally weighted regression smoother”.            often, less critical.
      Functions that use this approach include lowess(), loess(),
      loess.smooth(), and scatter.smooth().

  – Use of a regression spline basis in a linear model. Here the
      smoothness is usually controlled by the choice of number of
      spline basis terms.

• A second class of methods use a “statistical learning” approach
   in which the amount of smoothing is chosen automatically. The
   approach of the mgcv package extends and adapts the regression
   spline approach.8 The methodology generalizes to handle more
   general types of outcome variables, including proportions and
   counts. These extensions will not be further discussed here.

8.10.1 Locally weighted scatterplot smoothers

Locally weighted scatterplot smoothers pass a window across the

data, centering the window in turn at each of a number points that

are equally spaced through the data. The smooth at an x-value where

the window has been centred is the predicted value from a line (or

sometimes a quadratic or other curve) that is ﬁtted to values that

lie within the window. A weighted ﬁt is used, so that neighbouring

points get greater weight than points out towards the edge of the

window.

Figure 8.17 shows a smooth that has been ﬁtted using the lowess

(locally weighted scatterplot smoothing) methodology. The default

choice of with of smoothing window (a fraction  f  =  2  of the total
                                                      3
range of x) gives a result that, for these data, looks about right. The
174 learning and exploring r                                                       10000        q
                                                                                    8000
curve does however trend slightly upwards at the upper end of its                   6000           q
range. A monotonic response might seem more appropriate.                            4000                  q
                                                                                    2000
    The code used to plot the graph is:                                        ohms          qqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqq q                                                                                         q

## Plot points                                                                               q     q         q qqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqq
plot(ohms ~ juice, data=fruitohms , fg="gray")                                                       q
## Add smooth curve, using default
## smoothing window                                                                                                                                                                                                          q
with(fruitohms ,
                                                                                                                                                                                                                       q
         lines(lowess(ohms ~ juice), col="gray", lwd=2))
                                                                                                10 20 30 40 50 60
    A more sophisticated approach uses the gam() function in the
mgcv package. This allows automatic determination of the amount                                           juice
of smoothing, providing the assumption of independent residu-
als from the curve is reasonable. We now demonstrate the use of a       Figure 8.17: Resistance in ohms is
GAM model for a two-dimensional smooth.
                                                                        plotted against apparent juice content.
8.10.2 Contours from 2-dimensional Smooths
                                                                        A smooth curve (in gray) has been
Data are the amplitudes of responses to a visual stimulus, for each
of 20 individuals, at diﬀerent regions of the left eye. We use the      added, using the lowess smoother.
function gam() to create smooth surfaces, for males and females
separately. Figure 8.18 then uses the function vis.gam() to plot        The width of the smoothing window
heatmaps that show the contours:
                                                                        was the default fraction                                                                                                                       f  =  2     of the
                                                                                                                                                                                                                             3

                                                                        range of values of the x-variable.

  A: Response amplitudes, Males                                       B: Response amplitudes, Females

4                                                                   4
                                                                    2
                                                          12.5

2
y                                                               12
                        14 14.5
                                            13 13.5

                                                                       y

                                                                                                                                                   13 13.5
                                                                                                                                                                                   12 12.5
                                                                                                                                                                                                    11.5
                                                                                                                                                                                                                   11
00

−2 −2

−4                                                                  −4

    −4 −2  0  2                                                 4       −4 −2             0                  2                                                                                                                  4

           x                                                                              x

    The GAM ﬁt will as far as possible use the smooth surface to        Figure 8.18: Estimated contours of
account for the pattern of variation across the eye, with residuals     left eye responses to visual stimulae,
from the surface treated as random normal noise.                        projected onto the plane.

## Code
library(DAAGviz)
library(mgcv)
eyeAmpM.gam <- gam(amp ~ s(x,y), data=subset(eyeAmp , Sex=="m"))
eyeAmpF.gam <- gam(amp ~ s(x,y), data=subset(eyeAmp , Sex=="f"))
lims <- range(c(predict(eyeAmpF.gam), predict(eyeAmpM.gam)))
vis.gam(eyeAmpM.gam , plot.type='contour', color="cm", zlim=lims, main="")
mtext(side=3, line=0.5, adj=0, "A: Response amplitudes , Males")
                                                                               regression with linear terms and factors 175

  vis.gam(eyeAmpF.gam , plot.type='contour', color="cm", zlim=lims, main="")
  mtext(side=3, line=0.5, adj=0, "B: Response amplitudes , Females")

 8.11 Other important models and issues

 8.11.1 Errors in x

 In the classical "errors in x" model, one or more explanatory vari-
 ables are measured with error. In straight line regression, the eﬀect
 is to attenuate the regression coeﬃcient. If there is, additionally, a
 grouping factor with diﬀerent means for the outcome variable for
 diﬀerent groups, a side eﬀect of the attentuation is to make it appear
 that the lines are diﬀerent for the diﬀerent groups. The same general
 principles apply when curves are ﬁtted. Where several variables are
 measured with substantial error, there are increased oportunities for
 transferring estimated eﬀects between variables, and the mathemat-
 ics becomes more complicated.

     Examples on the help page ?DAAG::errorsINx show how to
 run simulations that demonstrate the eﬀects in the simple cases just
 described. These are useful in giving a sense of the magitutude of
 the attenuation, and any possible apparent group diﬀerence, that may
 arise from a given amount of measurement error.

     Comparison with data from a case where x is measured with
 minimal error is needed, in order to get a data-based indication of
 the extent of the error in x.

     An alternative to the classical "errors in x" model is the Berkson
 model. Consider an oven where the strength of pottery ﬁred in the
 oven is a linear function, for some ﬁxed time, of the temperature.

 - For the classical "errors in x" model, the measured temperature
    varies randomly about the oven temperature.

 - For the Berkson model, the oven temperature varies randomly
    about a temperature that is determined by setting on the tempera-
    ture control.

     In the Berkson case, the slope is an unbiased estimate of the true
 slope, but has increased standard error. In common situations, there
 may be a mix of classical errors and Berkson, with some additional
 bias.

 8.12 Exercises

1. Exercise 3 in Section 2.6.2 involved reading data into a data frame
    molclock1. Plot AvRate against Myr. Fit a regression line (with
    intercept, or without intercept?), and add the regression line to the
    plot. What interpretation can be placed upon the regression slope?
 176 learning and exploring r

2. Attach the DAAG package. Type help(elasticband) to see
    the help page for the data frame elasticband. Plot distance
    against stretch. Regress distance against stretch and ex-
    plain how to interpret the coeﬃcient.

3. Repeat the calculations in Section 8.6, now examining the regres-
    sion of time on dist and climb. Does this regression adequately
    model the data. Comment on the results.

4.(a) Investigate the pairwise relationships between variables in the
       data frame oddbooks (DAAG).

 (b) Fit the models

        volume <- apply(oddbooks[, 1:3], 1, prod)
        area <- apply(oddbooks[, 2:3], 1, prod)
        lob1.lm <- lm(log(weight) ~ log(volume), data=oddbooks)
        lob2.lm <- lm(log(weight) ~ log(thick)+log(area), data=oddbooks)
        lob3.lm <- lm(log(weight) ~ log(thick)+log(breadth)+log(height),

                                  data=oddbooks)

       Comment on what you ﬁnd, i.e., comment both on the estimates
       and on the standard errors.

 (c) Can weight be completely explained as a function of volume?
       Is there another relevant variable?

5. Repeat the calculations in Section 3.5, now with the dataset
    hills2000 (DAAG). Do any of the points stand out as outliers?
    Use predict(), with newdata = hills200, to obtain predic-
    tions from the hills2000 model for the nihills data. Compare
    with the predictions from the nihills model.
9
∗A Miscellany of Models & Methods
178 learning and exploring r

    This chapter is a tour through models and methods that are
straightforward to ﬁt using R. Some of these lend themselves to
relatively automated use. There is some limited attention to the traps
that can catch users who are unwary, or who have ventured too easily
into areas that call for some greater level of statistical sophistication
than their training and experience has given them.

    In each case, comments with be introductory and brief. Firstly,
there are brief comments on the ﬁtting of smooth curves. The ﬁrst
and second topics highlight speciﬁc types of departure from the iid
(independently and identically distributed) assumption.

9.1 Hierarchical Multilevel Models                                         ORAN                       q qqq
                                                                           WEAN
                                                                           DBAN                qq     qq
                                                                           OVAN
Models with Non-iid Errors – Multilevel models:                                                q qqq
 Error Term Errors do not have to be (and often are not) iid                LFAN
                                                                            TEAN          q    q qq
                                                                           WLAN
                                                                           NSAN        q       qq

                                                                                       qq q q

Multilevel  Multilevel models are a (relatively) simple type of                   q q qq
models      non-iid model, implemented using lme() (nlme) or
            lmer() (lme4 package).                                                qqq  q
            Such models allow diﬀerent errors of prediction,
            depending on the intended prediction.                                 234567

                                                                                  Harvest weight of corn

    Figure 9.1 shows corn yield data from the Caribbean island of          Figure 9.1: Yields from 4 packages
Antigua, as in the second column (“Yields”) of Table 9.1. Each value       of land on each of eight sites on the
is for one package of land. The code for the ﬁgure is:                     Caribbean island of Antigua. Data are
                                                                           a summarized version of a subset of
ant111b <- DAAG::ant111b                                                   data given in Andrews and Herzberg
Site <- with(ant111b , reorder(site, harvwt ,                              1985, pp.3˜39-353.

                                                      FUN=mean))
lattice::stripplot(Site ~ harvwt, data=ant111b ,

                  scales=list(tck=0.5),
                  xlab="Harvest weight of corn")

Location    Yields                Location            Residuals from       Table 9.1: The leftmost column has
                                  eﬀect                location mean       harvest weights (harvwt), for the
DBAN      5.16, 4.8, 5.07, 4.51                  0.28, −0.08, 0.18, −0.38  packages of land in each location, for
 LFAN     2.93, 4.77, 4.33, 4.8          +0.59    −1.28, 0.56, 0.12, 0.59  the Antiguan corn data. Each of these
NSAN      1.73, 3.17, 1.49, 1.97          −0.08  −0.36, 1.08, −0.6, −0.12  harvest weights can be expressed as
ORAN      6.79, 7.37, 6.44, 7.07          −2.2   −0.13, 0.45, −0.48, 0.15  the sum of the overall mean (= 4.29),
                                  (4.29) +2.62                             location eﬀect (third column), and
OVAN      3.25, 4.28, 5.56, 6.24         +0.54   −1.58, −0.56, 0.73, 1.4   residual from the location eﬀect (ﬁnal
 TEAN     2.65, 3.19, 2.79, 3.51          −1.26  −0.39, 0.15, −0.25, 0.48  column).
WEAN      5.04, 4.6, 6.34, 6.12          +1.23   −0.49, −0.93, 0.81, 0.6
WLAN      2.02, 2.66, 3.16, 3.52          −1.45  −0.82, −0.18, 0.32, 0.68

    Depending on the use that will be made of the results, it may be

essential to correctly model the structure of the random part of the
model. In comparing yields from diﬀerent packages of land, there
are two sorts of comparison. Packages on the same location should
be relatively similar, while packages on diﬀerent locations should
be relatively more diﬀerent, as Figure 9.1 suggests. A prediction for
                                                                      a miscellany of models & methods 179

a new package at one of the existing locations is likely to be more     Because of the balance the corn yield
accurate than a prediction for a totally new location.                  data, an analysis of variance that
                                                                        speciﬁes a formal Error term is an
    Multilevel models are able to account for such diﬀerences in        alternative to the ﬁtting of a multilevel
predictive accuracy. For the Antiguan corn yield data, it is necessary  model.
to account both for variation within sites and for variation between
sites. The R packages nlme and lme4 are both able to handle such
data.

9.2 Regular Time Series in R

Models with Non-iid Errors – Time Series:

Time       Points that are close together in time commonly show

sequential a (usually, +ve) correlation. R’s acf() and arima()

           functions are powerful tools for use with time series.

    Any process that evolves in time is likely to have a sequential
correlation structure. The value at the current time is likely to be
correlated with the value at the previous time, and perhaps with
values several time points back. The discussion that follows will
explore implications for data analysis.

9.2.1 Example – the Lake Erie data

Level (m)  174.5                                                        Figure 9.2: Lake Erie levels (m).
           174.0
           173.5                                                        Erie <- DAAG::greatLakes[,"Erie"]
                                                                        plot(Erie, xlab="", fg="gray",
                        1920
                                                                                 ylab="Level (m)")

                              1940  1960   1980  2000

    The data series Erie, giving levels of Lake Erie from 1918 to       1 Data are from http://www.lre.
                                                                        usace.army.mil/greatlakes/
2009, will be used as an example from which to start the discus-        hh/greatlakeswaterlevels/
sion.1 The series is available in the DAAG package, as the column       historicdata/
                                                                        greatlakeshydrographs/
Erie in the multivariate time series object greatLakes.

    Figure 9.2 shows a plot of the series.
180 learning and exploring r

          173.5              174.0        174.5                                     175.0                                                                                    173.5              174.0                    174.5         175.0  Figure 9.3: Panel A plots Lake Erie
                                                                                                                                                                                                                                              levels vs levels at lags 1, 2 and 3 re-
                                                                             q                                                      q                                                                           q                             spectively. Panel B shows a consistent
                                                                                                                                                                                                                                              pattern of decreasing autocorrelation
                                       q qq q                                       q                                        q                              qq                                      q           q              q              at successive lags.
                                                                                                                                                          q                                     q                qq         q qq
                                                                                                                                 q                                                                                    q                       ## Panel A
                                                                                                                                                                 qq                                                    q                      lag.plot(Erie, lags=3,
                                          q                               q                                                         q                                                                                qq
                                            qqq q                                                                                        q                                                                                                                    do.lines =FALSE ,
174.5                               q                                           qq                                        q         qq q                             q                                                                 q                      layout=c(1,3), fg="gray",
174.0
173.5                               qqqqqqqqqqqqqqqqqqqqqqqqqqqqq  qqqqq        q                                         q      qqqqqqqqqqqqqqqqqqqqqqq   qqqq       175.0                q             q         qq   qq  qq     q          ## Panel B
                                         q                                                                                q                               q                                               q            q    qqq               acf(Erie, main="", fg="gray")
                                                                                                       q                                                  q      qq                     q    q       q                            q
                    q                qq q                                                                q                                                                           q       q             q          q      q   q
                                 q                                                                                     q                                   q                                                    q qqq
                          q  q                                                                                                                                                          q                    q                      q
                     qq      q qq                                                                       q q qq                                                                        q               qq         qq
     Erie           q q qqq   qq                                                                          q qq                                                                    q             q    qq q       qq
                             q                                                                                                                                                    q               q
                           ACF                                                                                 q qq          q q qq                                                               q         q
                                                                                                       q q qq
                                                      Erieqqqq                                    q                          qq  q                                                   qq                q               q
                                                                                                        Erieqqq              q                                                       q qq qq                q
                 q                                                                                                                     q
                                                                                                                                                                                                       q qq
                       q                                                                          q qq                                                                              q q qq
       q         q qq    qq                                                                q                                 qq                                              qqq
                                                                                               q
                    qq q                                                                                q qq                                                                                                       q
                                                                                                                                                                                                qq

                       q                                                                                               q                                                                    q
                                                                                                                      q                                                                     q
              q                                                                                   q                                                                                        q
          q

                             lag 1                                                         173.5                          lag 2           174.5                                                 lag 3
                                                                                                                          174.0

                                           1.0                                                         5 10                                                                  15
                                           0.8
                                           0.6                                                                       Lag
                                           0.4
                                           0.2
                                           0.0
                                          −0.2

                                                     0

    The plots in Figure 9.3 are a good starting point for investigation                                                                                                                                                                       Where values of covariates are
of the correlation structure. Panel A shows lag plots, up to a lag of                                                                                                                                                                         available that largely or partly explain
3. Panel B shows estimates of the successive correlations, in this                                                                                                                                                                            the dependence, it may make sense
context are called autocorrelations.                                                                                                                                                                                                          to account for «««< HEAD these
                                                                                                                                                                                                                                              in the model. The details of how
    There is a strong correlation at lag 1, a strong but weaker correla-                                                                                                                                                                      this should be done will =======
tion at lag 2, and a noticeable correlation at lag 3. Such a correlation                                                                                                                                                                      these in the model. THe details of
pattern is typical of an autoregressive process where most of the                                                                                                                                                                             how this should be done will »»»>
sequential dependence can be explained as a ﬂow-on eﬀect from a                                                                                                                                                                               5245dac2bad38cd7565fda39666c5c0ﬀ90565f8
dependence at lag 1.                                                                                                                                                                                                                          depend on the intended use of the
                                                                                                                                                                                                                                              model.
    In an autoregressive time series, an independent error component,
or “innovation” is associated with each time point. For an order p                                                                                                                                                                            An autoregressive model is a special
autoregressive time series, the error for any time point is obtained                                                                                                                                                                          case of an Autoregressive Moving
by taking the innovation for that time point, and adding a linear                                                                                                                                                                             Average (ARMA) model.
combination of the innovations at the p previous time points. (For
the present time series, initial indications are that p = 1 might
capture most of the correlation structure.)

9.2.2 Patterns that are repeatable                                                                                                                                                                                                            Smoothing terms can be ﬁtted to the
                                                                                                                                                                                                                                              pattern apparent in serially correlated
What sorts of patterns may then be repeatable? Indications that a                                                                                                                                                                             data, leaving errors that are pretty
pattern may be repeatable include:                                                                                                                                                                                                            much uncorrelated. Such a pattern is
                                                                                                                                                                                                                                              in general, however, unrepeatable. It
• A straight line trend is a good starting point for some limited                                                                                                                                                                             gives little clue of what may happen
   extrapolation. But think: Is it plausible that the trend will continue                                                                                                                                                                     the future. A re-run of the process
   more than a short distance into the future?                                                                                                                                                                                                (a new realization) will produce a
                                                                                                                                                                                                                                              diﬀerent series, albeit one that shows
• There may be a clear pattern of seasonal change, e.g., with seasons                                                                                                                                                                         the same general tendency to move up
   of the year or (as happens with airborne pollution) with days of                                                                                                                                                                           and down.
   the week. If yearly seasonal changes persist over diﬀerent years,
                                                                                        a miscellany of models & methods 181

   or weekly day-of-the-week changes persist over diﬀerent weeks,
   these eﬀects can perhaps be extrapolated with some reasonable
   conﬁdence.

• There is a regression relationship that seems likely to explain
   future as well as current data.

    An ideal would be to ﬁnd a covariate or covariates than can
largely explain the year to year changes. For this series, this does
not seem a possibility. In the absence of identiﬁable direct cause for
the year to year changes, a reasonable recourse is to look for a cor-
relation structure that largely accounts for the pattern of the year to
year change.

Smooth, with automatic choice of smoothing parameter

                                                                               q                          Figure 9.4: GAM smoothing term,
                                                                                                          ﬁtted to the Lake Erie Data. Most of
Height of lake  174.5                                q            qq          qq       q                  the autocorrelation structure has been
                174.0                                               qq  q qq      qq                      removed, leaving residuals that are
                173.5                                                                                     very nearly independent.
                                                                 q
                                                                         qqqq                             ## Code
                                   qq       qqqqqqqqqqqqq qq        q   q         qqqqqqqq             q  library(mgcv)
                       q                                           qqq                         qqqqqq     df <- data.frame(
                                                                  q                         q
                       qq                                        q                                           height=as.vector(Erie),
                       qq q                       q                                         qqq              year=time(Erie))
                                                        qq q                                              obj <- gam(height ~ s(year),
                            q  q  qqq  qqqqq
                           q                                                                                                  data=df)
                                            q qq                                                          plot(obj, fg="gray",
                             qq
                                                              q                                                    shift=mean(df$height),
                                                                                                                   residuals=TRUE, pch=1,
                                         q                                                                         xlab="",
                                        q                                                                          ylab="Height of lake")
                                       q

                       1920                 1940        1960            1980                2000

    While smoothing methods that asssume independent errors can
be used, as in Figure 9.4, to ﬁt a curve to such data, the curve will
not be repeatable. Figure 9.4 does not separate systematic eﬀects
from eﬀects due to processes that evolve in time. Figure 9.4 uses
the abilities of the mgcv package, assuming independently and iden-
tically distributed data (hence, no serial correlation!) to make an
automatic choice of the smoothing parameter. As the curve is condi-
tional on a particular realization of the process that generated it, its
usefulness is limited.

    The pointwise conﬁdence limits are similarly conditioned, rel-
evant perhaps for interpolalation given this particular realization.
All that is repeatable, given another realization, is the process that
generated the curve, not the curve itself.

9.2.3 Fitting and use of an autoregressive model

There are several diﬀerent types of time series models that may be
used to model the correlatios structure, allowing realistic estimates
of the lake level a short time ahead, with realistic conﬁdence bounds
around those estimates. For the Lake Erie data, an autoregressive
182 learning and exploring r

correlation structure does a good job of accounting for the pattern of
change around a mean that stays constant.

    Figure 9.3 suggested that a correlation between each year and
the previous year accounted for the main part of the autocorrela-
tion structure in Figure 9.2. An AR1 model (autoregressive with a
correlation at lag 1 only), which we now ﬁt, formalizes this.

ar(Erie, order.max=1)

Call:
ar(x = Erie, order.max = 1)

Coefficients:
       1

0.851

Order selected 1 sigma∧2 estimated as 0.0291

The one coeﬃcient that is now given is the lag 1 correlation,
equalling 0.851.

Sim 1   4                       Sim 2   4                       Sim 3   4                       Figure 9.5: The plots are from re-
        2                               2                               2                       peated simulations of an AR1 process
        0       50 100 150 200          0       50 100 150 200          0       50 100 150 200  with a lag 1 correlation of 0.85.
       −2                              −2                              −2                       Smooth curves, assuming independent
       −4                x             −4                x             −4                x      errors, have been ﬁtted.
       −6                              −6
                50 100 150 200                                               0  50 100 150 200  for (i in 1:6){
             0                               0                                                  ysim <-
                         x                                              4                x
Sim 4   4                       Sim 5   2                       Sim 6   2                          arima.sim(list(ar=0.85),
        2                               0                               0                                            n=200)
        0                              −2       50 100 150 200         −2
       −2                              −4                              −4                       df <- data.frame(x=1:200,
       −4                                                x                                                                     y=ysim)
                                             0                               0
             0                                                                                  df.gam <- gam(y ~ s(x),
                                                                                                                         data=df)
    Figure 9.5 then investigates how repeated simulations of this
process, with a lag 1 correlation of 0.0.85, compare with Figure 9.2.                           plot(df.gam, fg="gray",
This illustrates the point that a GAM smooth will extract, from an                                       ylab=paste("Sim", i),
autoregressive process with mean 0, a pattern that is not repeatable                                     residuals=TRUE)
when the process is re-run.
                                                                                                }
    The curves are diﬀerent on each occasion. For generalization
beyond the particular realization that generated them, they serve no
useful purpose.

    Once an autoregressive model has been ﬁtted, the function
forecast() in the forecast package can be used to predict future
levels, albeit with very wide conﬁdence bounds. For this, it is neces-
sary to reﬁt the model using the function arima(). An arima model
with order (1,0,0) is an autoregressive model with order 1.
                                                                                                                                                                                                                                                a miscellany of models & methods 183

Lake level (m)  174.5                                                                                                                                                                                                                               Figure 9.6: Predictions, 15 years
                                                                                                                                                                                                                                                    into the future, of lake levels (m).
                174.0                                                                                                                                                                                                                               The shaded areas give 80% and 95%
                                                                                                                                                                                                                                                    conﬁdence bounds.
                173.5                    1940                                                                               1960  1980        2000          2020
                             1920                                                                                                                                                                                                                   erie.ar <- arima(Erie,
                                                                                                                                                                                                                                                                          order=c(1,0,0))

                                                                                                                                                                                                                                                    library(forecast)
                                                                                                                                                                                                                                                    fc <- forecast(erie.ar ,

                                                                                                                                                                                                                                                                               h=15)
                                                                                                                                                                                                                                                    plot(fc, main="", fg="gray",

                                                                                                                                                                                                                                                             ylab="Lake level (m)")
                                                                                                                                                                                                                                                       # 15 time points ahead

    This brief excursion into a simple form of time series model is
intended only to indicate the limitations of automatic smooths. and
to give a sense of the broad style of time series modeling. The list of
references at the end of the chapter has details of several books on
time series.

9.2.4 Regression with time series errors

Figure 9.7 ﬁts annual rainfall, in the Murray-Darling basin of Aus-
tralia, as a sum of smooth functions of Year and SOI. Figure 3.9
shows the estimated contributions of the two model terms.

## Code
bomregions <- DAAG::bomregions2015
mdbRain.gam <- gam(mdbRain ~ s(Year) + s(SOI),

                                  data=bomregions)
plot(mdbRain.gam , residuals=TRUE, se=2, fg="gray",

         pch=1, select=1, cex=1.35, ylab="Partial , Year")
mtext(side=3, line=0.75, "A: Effect of Year", adj=0)
plot(mdbRain.gam , residuals=TRUE, se=2, fg="gray",

         pch=1, select=2, cex=1.35, ylab="Partial , SOI")
mtext(side=3, line=0.75, "B: Effect of SOI", adj=0)

                A: Effect of Year                                                                                                 B: Effect of SOI                                                                                                  Figure 9.7: Estimated contributions of
                                                                                                                                                                                                                                                    model terms to mdbRain, in a GAM
300                                                                                                                               300 qq q                                                                                                          model that adds smooth terms in Year
                                                                                                                                                                                                                                                    and Rain. The dashed curves show
                                         qq                                                                                                            q                                                                                            pointwise 2-SE limits, for the ﬁtted
                                     q                                                                                                                                                                                                              curve.
200                                q qq q                                                                                          200                      q                                                                                   q
Partial, Year                                                                                                                      100                 qqq  q
                                                   Partial, SOI                                                                            q  q
                                                                                                                                       0q
 100            qqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqq        −100     qqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqq
     0                                                                                                                            −200

−100

−200 q                 qq

                1900   1940              1980                                                                                     −40 −20           0       20                                                                                  40

                                   Year                                                                                                       SOI

The left panel indicates a consistent pattern of increase of rainfall
184 learning and exploring r

with succeeding years, given an adjustment for the eﬀect of SOI. Er-
rors from the ﬁtted model are consistent with the independent errors
assumption. The model has then identiﬁed a pattern of increase of
rainfall with time, given SOI, that does seem real. It is necessary to
warn against reliance on extrapolation more than a few time points
into the future. While the result is consistent with expected eﬀects
from global warming, those eﬀects are known to play out very diﬀer-
ently in diﬀerent parts of the globe.

Investigation of the residual error structure

Sequential correlation structures are often eﬀective, with data col-
lected over time, for use in modeling departure from iid errors.
Where there is such structure structure in the data, the methodol-
ogy will if possible use a smooth curve to account for it.

    The residuals can be checked to determine whether the ﬁtted
curve has removed most of the correlation structure in the data. Fig-
ure 9.8 shows the autocorrelation function of the residuals, followed
by autocorrelation functions for several series of independent ran-
dom normal numbers. Apart from the weakly attested correlation
at a lag of 12 years, which is a commonplace of weather data, the
pattern of sequential correlation is not much diﬀerent from what can
be expected in a sequence of independent random normal numbers.

MDB series   1.0                             Sim 1   1.0                             Sim 2   1.0
             0.8                                     0.8                                     0.8
             0.6        Series rnorm(n)  20          0.6        Series rnorm(n)  20          0.6        Series rnorm(n)  20
             0.4                                     0.4                                     0.4
             0.2           5 10 15                   0.2           5 10 15                   0.2           5 10 15
             0.0                                     0.0                                     0.0
            −0.2                  Lag               −0.2                  Lag               −0.2                  Lag

Sim 3                0                       Sim 4           0                       Sim 5           0

             1.0        5 10 15 20                   1.0        5 10 15 20                   1.0        5 10 15 20
             0.8                                     0.8                                     0.8
             0.6                                     0.6                                     0.6
             0.4                                     0.4                                     0.4
             0.2                                     0.2                                     0.2
             0.0                                     0.0                                     0.0
            −0.2                                    −0.2                                    −0.2

                     0                                       0                                       0

    Code is:                                                                                Figure 9.8: The top left panel shows
                                                                                            the autocorrelations of the residu-
mdbRain.gam <- gam(mdbRain ~ s(Year) + s(SOI),                                              als from the model mdbRain.gam.
                                  data=bomregions)                                          The ﬁve remaining panels are the
                                                                                            equivalent plots for sequences of
n <- dim(bomregions)[1]                                                                     independent random normal numbers.
acf(resid(mdbRain.gam), ylab="MDB series")
for(i in 1:5)acf(rnorm(n), ylab=paste("Sim",i),

                              fg="gray", col="gray40")
                                                            a miscellany of models & methods 185

9.2.5 ∗Box-Jenkins ARIMA Time Series Modeling                                         Models that are closely analagous to
                                                                                      ARIMA models had been used earlier
From the perspective of the Box-Jenkins ARIMA (Autoregressive                         in control theory. ARIMA models are
Integrated Moving Average) approach to time series models, au-                        feedback systems!
toregressive models are a special case. Many standard types of time
series can be modeled very satisfactorily as ARIMA processes.

Exercise

The simulations in Figure 9.5 show a pattern of variation that seems
not too diﬀerent from that in the actual series. Modeling of the pro-
cess as an ARMA or ARIMA process (i.e., allow for a moving av-
erage term) may do even better. Use the auto.arima() function in
the forecast package to ﬁt an ARIMA process:

9.2.6 Count Data with Poisson Errors                                                  Data are a time series. Serious acci-
                                                                                      dents are however suﬃciently uncom-
Data is for aircraft accidents, from the website http://www.                          mon that occasions where events occur
planecrashinfo.com/. The 1920 ﬁle has accidents starting from                         together, or where one event changes
1908. The full data are in the dataset gamclass::airAccs. Such                        the probability of the next event, seem
issues as there are with sequential correlation can be ameliorated by                 likely to be uncommon.
working with weekly, rather than daily, counts.

Estimated rate per week  2.0
                         1.5
                         1.0
                         0.5

                              2006  2007  2008  2009  2010  2011                      2012  2013  2014

    Figure 9.9 shows a ﬁtted smooth curve, with pointwise conﬁ-                       Figure 9.9: Estimated number of
dence bounds, from a GAM smoothing model that was ﬁtted to the                        events (aircraft crashes) per week,
weekly counts.                                                                        versus time. The yearly tick marks are
                                                                                      for January 1 of the stated year.
    The function gamclass::eventCounts() was used to create                           See Section 4.3.9 for further details on
weekly counts of accidents from January 1, 2006:                                      the function eventCounts().

## Code
airAccs <- gamclass::airAccs
fromDate <- as.Date("2006-01-01")
dfWeek06 <- gamclass::eventCounts(airAccs , dateCol="Date",

                                                              from=fromDate ,
                                                          by="1 week", prefix="num")
dfWeek06$day <- julian(dfWeek06$Date, origin=fromDate)

    Code for Figure 9.9 is then.
186 learning and exploring r

## Code
library(mgcv)
year <- seq(from=fromDate , to=max(dfWeek06$Date), by="1 year")
at6 <- julian(seq(from=fromDate , to=max(dfWeek06$Date), by="6 months"), origin=fromDate)
atyear <- julian(year, origin=fromDate)
dfWeek06.gam <- gam(num~s(day, k=200), data=dfWeek06 , family=quasipoisson)
avWk <- mean(predict(dfWeek06.gam))
plot(dfWeek06.gam , xaxt="n", shift=avWk, trans=exp, rug=FALSE,

         xlab="", ylab="Estimated rate per week", fg="gray")
axis(1, at=atyear , labels=format(year, "%Y"), lwd=0, lwd.ticks=1)
abline(h=0.5+(1:4)*0.5, v=at6, col="gray", lty=3, lwd=0.5)

    The argument ‘k’ to the function s() that sets up the smooth
controls the temporal resolution. A large k allows, if the data seem
to justify it, for ﬁne resolution. A penalty is applied that discrimi-
nates against curves that are overly “wiggly”.

    Not all count data is suitable for modeling assuming a Pois-
son type rare event distribution. For example, the dataset http:
//maths-people.anu.edu.au/~johnm/stats-issues/data/
hurric2014.csv has details, for the years 1950-2012, of US deaths
from Atlantic hurricanes. For any given hurricane, deaths are not at
all independent rare events.

9.3 Classiﬁcation                                                       For the special case g = 2, logistic
                                                                        regression models are an alternative.
Classiﬁcation models have the character of regression models where
the outcome is categorical, one of g classes. The fgl (forensic glass)  2 Limited further details and refer-
dataset that will be used as an example has measurements of each on     ences are provided in Maindonald and
nine physical properties, for 214 samples of glass that are classiﬁed   Braun: Data Analysis and Graphics
into g = 6 diﬀerent glass types.                                        Using R, Cambridge University Press,
                                                                        3rd edn 2010.
    This section will describe a very limited range of available ap-
proaches. For details on how and why these methods work, it will be
necessary to look elsewhere.2

    Linear discriminant analysis (LDA), and quadratic discriminant
analysis (QDA) which slightly generalizes LDA, both use linear
functions of the explanatory variables in the modeling of the proba-
bilities of group membership. These methods will be contrasted with
the strongly non-parameteric approaches of tree-based classiﬁcation
and of random forests.

Linear and quadratic discriminant analysis                              3 Quadratic discriminant analysis is
                                                                        an adaptation of linear discriminant
The functions that will be used are lda() and qda(), from the           analysis to handle data where the
MASS package. The function lda() implements linear discriminant         variance-covariance matrices of
analysis, while qda() implements quadratic discriminant analysis.3      the diﬀerent classes are markedly
                                                                        diﬀerent.
library(MASS, quietly=TRUE)

    Results from use of lda() lead very naturally to useful and
informative plots. Where lda() gives results that are a sub-optimal
ﬁt to the data, the plots may hint at what type of alternative method
a miscellany of models & methods 187

may be preferable. They may identify subgroups of the orginal g           4 This is based on a spectral decompo-
groups, and/or identify points that seem misclassiﬁed.                    sition of the model matrix.

    An attractive feature of lda() is that the discriminant rule that is  With three groups, two dimensions
                                                                          will account for all the variation. A
obtained has a natural representation r-dimensional space. Providing      scatterplot is then a geometrically
that there is suﬃcient independent covariate data, r = g − 1. The         complete representation of what the
analysis leads4 to r sets of scores, where each set of scores explains    analysis has achieved.

a successively smaller (or at least, not larger) proportion of the sum
of squares of diﬀerences of group means from the overall mean.
The r sets of scores can be examined using a pairs plot. With larger

numbers of groups, it will often happen that two or at most three

dimensions will account for most of the variation.

Use of lda() to analyse the forensic glass data

As noted above, the data frame fgl has 10 measured physical char-
acteristics for each of 214 glass fragments that are classiﬁed into
6 diﬀerent types. First, ﬁt a linear discriminant analysis, and use
leave-one-out cross-validation to check the accuracy, thus:

fglCV.lda <- lda(type ~ ., data=fgl, CV=TRUE)
tab <- table(fgl$type, fglCV.lda$class)
## Confusion matrix
print(round(apply(tab, 1, function(x)x/sum(x)),

                      digits=3))

                WinF WinNF Veh Con Tabl Head
   WinF 0.729 0.237 0.647 0.000 0.111 0.034
   WinNF 0.229 0.684 0.353 0.462 0.222 0.069
   Veh 0.043 0.000 0.000 0.000 0.000 0.000
   Con 0.000 0.039 0.000 0.462 0.000 0.034
   Tabl 0.000 0.026 0.000 0.000 0.556 0.000
   Head 0.000 0.013 0.000 0.077 0.111 0.862

    The function confusion() (DAAG) makes it easy to get all the
above output. Enter:

library(DAAG)
confusion(fgl$type, fglCV.lda$class)

Two-dimensional representation                                            Observe that most of the discrimina-
                                                                          tory power is in the ﬁrst two dimen-
Now ﬁt the model with CV=FALSE, which is the default:                     sions.

fgl.lda <- lda(type ~ ., data=fgl)

The ﬁnal three lines of the output, obtained by entering fgl.lda at
the command line, are:

Proportion of trace:
   LD1 LD2 LD3 LD4 LD5

0.815 0.117 0.041 0.016 0.011

The numbers show the successive proportions of a measure of the
188 learning and exploring r

                4                                                          Figure 9.10: Visual representation
                                                                           of scores from a linear discriminant
                2                                                          analysis, for the forensic glass data. A
                                                                           six-dimensional pattern of separation
Discriminant 2  0                                 WinF                     between the categories has been
                                                  WinNF                    collapsed to two dimensions. Some
                −2                                Veh                      categories may therefore be better
                                                  Con                      distinguished than is evident from this
                                                  Tabl                     ﬁgure.
                                                  Head
                −4

                −6

                −8       −2 0 2 4              6
                     −4
                               Discriminant 1

variation that are accounted for by projections onto spaces with           See Figure 9.13 in Subsection 9.4.1,
successively larger numbers of dimensions.                                 for an example of the type of low-
                                                                           dimensional representation that is
    Figure 9.10 shows the two-dimensional representation.                  possible for results from a randdom
                                                                           forest classiﬁcation.
library(lattice)
scores <- predict(fgl.lda)$x
xyplot(scores[,2] ~ scores[,1], groups=fgl$type,

            xlab="Discriminant 1",
            ylab="Discriminant 2",
            aspect=1, scales=list(tck=0.4),
            auto.key=list(space="right"),

    Additionally, it may be useful to examine the plot of the third
versus the second discriminant. Better still, use the abilites of the rgl
package to examine a 3-dimensional dynamic representation. With
most other methods, a low-dimensional representation does not arise
so directly from the analysis.

Two groups – comparison with logistic regression                           More technical points, as they apply
                                                                           to the use of R’s function glm() for
The approach is to model the probability of class membership given         logistic regression, are:
the covariates, using the same logistic ﬁxed part of the model as
for linear and quadratic discriminant analysis. With π equal to the        - The ﬁtting procedure minimizes
probability of membership in the second class, the model assumes              the deviance. This equals 2 (
that                                                                          loglikelihood for ﬁtted model,
                                                                              minus the loglikelihood for the
                           log(π/(1 − π) = β x                                ‘saturated’ model). The ‘saturated
                                                                              model has predicted values equal to
where β is a vector of coeﬃcients that are to be estimated, and x is a        observed values.
vector of covariate values.
                                                                           - Standard errors and Wald statistics
    A logistic regression model is a special case of a Generalized            (roughly comparable to t-statistics)
Linear Model (GLM), as implemented by R’s function glm(). There               are given for parameter estimates.
is no provision to adjust predictions to take account of prior proba-         These depend on approximations
bilities, though this can be done as an add-on to the analysis. Other         that may fail if predicted propor-
points of diﬀerence from linear discriminant analysis are:                    tions are close to 0 or 1 and/or the
                                                                              sample size is small.
                                                                                        a miscellany of models & methods 189

• Inference is conditional on the observed covariate values. A model
   for the probability of covariate values x given the class c, as for
   linear discriminant analysis, is not required. (Linear discriminant
   analysis assumes a multivariate normal distribution assumptions
   for x, given the class c. In practice, results seem relatively robust
   against failure of this assumption.)

• The logit model uses the link function f (π) = log(π/(1 − π).
   Other choices of link function are available. Where there are suf-
   ﬁcient data to check whether one of these other links may be more
   appropriate, this should be checked. Or there may be previous
   experience with comparable data that suggests use of a link other
   than the logit.

• Observations can be given prior weights.

9.4 Tree-based methods and random forests                             The dataset bronchit may alterna-
                                                                      tively be found in the SMIR package.
On a scale in which highly parametric methods lie at one end and      Here r=1 denotes bronchitis, while
highly non-parametric methods at the other, linear discriminant       r=0 indicates that bronchitis is absent.
methods lie at the parametric end, and tree-based methods and ran-
dom forests at the non-parametric extreme. An attraction of tree-     With a factor (rfac) as outcome,
based methods and random forests is that model choice can be pretty   method="class" is the default.
much automated.                                                       Setting method="class", to make
                                                                      it quite clear that we are using a
    We begin by loading the rpart package:                            splitting rule that is appropriate to a
                                                                      categorical (rather than continuous)
library(rpart)                                                        outcome, is good practice.

    For the calculations that follow, data are columns in the data
frame bronchit, in the DAAGviz package.

bronchit <- DAAGviz::bronchit
head(bronchit , 3)

   r cig poll
1 0 5.15 67.1
2 0 6.75 64.4
3 0 0.00 65.9

In place of the variable r with values 0 and 1, we use a factor with
levels abs and pres. Labels that appear in the output are then more
meaningful.

## Now make the outcome variable a factor
bronchit <-

   within(bronchit ,
                rfac <- factor(r, labels=c("abs","pres")))

    The following ﬁts a tree-based model:

set.seed(47) # Reproduce tree shown
b.rpart <- rpart(rfac ~ cig+poll, data=bronchit ,

                               method="class")
190 learning and exploring r                                                                                 cig< 4.375
                                                                                                                    |
    The “complexity” paremeter cp, by default set to 0.01, controls
how far splitting continues. In practice, it is usual to set cp small                                                        cig< 6.3
enough that splitting continues further than is optimal, then pruning                               abs
the tree back. Cross-validation based accuracies are calculated at
each split, and can be used to determine the optimal depth of tree.                                                                      poll< 58.35
Details will not be given at this point, as the interest is in trees as a                                        abs poll>=55.65
lead-in to random forests. For random forests, the depth of the splits
in individual trees is not of great consequence — it is not important                                                                                  pres
for individual trees to be optimal.                                                                                          abs pres

    Figure 9.11 is a visual summary of results from the tree-based                              Figure 9.11: Decision tree for predict-
classiﬁcation, designed to predict the probability that a miner will                            ing whether a miner has bronchitis.
have bronchitis. Where the condition at a node is satisﬁed, the left                            Code for Figure 9.11 is:
branch is taken. Thus, at the initial node, cig<4.385 takes the
branch to the left. In general (no random number seed), the tree may                            plot(b.rpart)
be diﬀerent for each diﬀerent run of the calculations.                                          text(b.rpart, xpd=TRUE)

    Tree-based classiﬁcation proceeds by constructing a sequence of                             Figure 9.12: Each tree is for a diﬀerent
decision steps. At each node, the split is used that best separates the                         bootstrap sample of observations. The
data into two groups. Here (Figure 9.11) tree-based regression does                             ﬁnal classiﬁcation is determined by
unusually well (CV accuracy = 97.2%), perhaps because it is well                                a random vote over all trees. Where
designed to reproduce a simple form of sequential decision rule that                            there are > 2 explanatory variables
has been used by the clinicians.                                                                (but not here) a diﬀerent random
                                                                                                sample of variables is typically used
    How is ‘best’ deﬁned? Splits are chosen so that the Gini index of                           for each diﬀerent split. The ﬁnal
“impurity” is minimized. Other criteria are possible, but this is how                           classiﬁcation is determined by a
randomForest() constructs its trees.                                                            random vote over all trees.

9.4.1 Random forests

library(randomForest , quietly=TRUE)

    Figure 9.12 shows trees that have been ﬁtted to diﬀerent boot-
strap samples of the bronchitis data. Typically 500 or more trees are
ﬁtted, without a stopping rule. Individual trees are likely to overﬁt.
As each tree is for a diﬀerent random sample of the data, there is no
overﬁtting overall.

     cig< 3.3                cig< 6.3                cig< 6.3                cig< 6.4

             |                       |                       |                       |

           poll< 53.95             poll< 57.65             poll< 53.95              poll< 58.5
abs                     abs                     abs                     abs

          abs pres                abs pres                abs pres                abs pres
                                cig< 6.3              cig< 6.725             cig< 3.3
     cig< 6.4
                                        |                       |                    |
             |
                        abs pres                abs pres                           poll< 54.55
           poll< 58.35      cig< 3.45               cig< 3.45           abs
abs
                                     |                       |                    abs pres
          abs pres                                                            cig< 6.725
                                    cig< 7.325             poll< 61.45
    cig< 4.625          abs                     abs                                     |

             |                    abs pres                abs pres      abs pres

           poll< 54.55
abs

          abs pres
                                                                                       a miscellany of models & methods 191

    For each bootstrap sample, predictions are made for the obser-
vations that were not included – i.e., for the out-of-bag data. Com-
parison with the actual group assignments then provides an unbiased
estimate of accuracy.

    For the bronchit data, here is the randomForest() result.

(bronchit.rf <- randomForest(rfac ~ cig+poll,
                                                    data=bronchit))

Call:
 randomForest(formula = rfac ∼ cig + poll, data = bronchit)
                           Type of random forest: classification
                                      Number of trees: 500

No. of variables tried at each split: 1

OOB estimate of error rate: 23.58%                                                                   abs q                         pres

Confusion matrix:

abs pres class.error                                                                                 qqqqqqqqqqqqqqqqqqq

abs 145 21         0.1265                                                        0.2                                          q    qqqqqqqqqqqqqqqqqqqqqqqqqq
                                                                                 0.0
                                                                                −0.2                                      qq
                                                                                −0.4
pres 29 17         0.6304

                                                                        Axis 2                                                     q   q                       qqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqq
                                                                                                                                    q  q

The accuracy is much better than the rpart() accuracy. The ran-                       qqqqqqqqqqqqq
dom forest methodology will often improve, sometimes quite dra-
matically, on tree-based classiﬁcation.                                               qqqqq                               qqqqqqq
                                                                                      qqq q
    Figure 9.13 is a visual summary of the random forest classiﬁ-                                    q
cation result. The proportion of trees in which any pair of points
appear together at the same node may be used as a measure of the                                     qq qqqqqqqqqqqq
“proximity” between that pair of points. Then, subtracting proxim-
ity from one to obtain a measure of distance, an ordination method                    −0.4 −0.2 0.0 0.2                                                        0.4
is used to ﬁnd an approximates representation of those points in a
low-dimensional space.                                                                             Axis 1

    There is a tuning parameter mtry which controls the number          Figure 9.13: The plot is designed
of randomly chosen variables considered for each tree. This is not      to represent, in two dimensions, the
too much of an issue for the present data, where there are only two     random forest result. It aims to reﬂect
explanatory variables.                                                  probabilities of group membership
                                                                        given by the analysis. It is not derived
    Code for Figure 9.13 is:                                            by a ’scaling’ of the feature space.

parset <- simpleTheme(pch=1:2)
bronchit.rf <- randomForest(rfac ~ cig+poll,

                                                   proximity =TRUE ,
                                                   data=bronchit)
points <- cmdscale(1-bronchit.rf$proximity)
xyplot(points[,2] ~ points[,1],
            groups = bronchit $rfac ,
            xlab="Axis 1", ylab="Axis 2",
            par.settings=parset , aspect=1,
            auto.key=list(columns=2))

A random forest ﬁt to the forensic glass data                           5 The default is to set mtry to the
                                                                        square root of the total number of
The algorithm can be used in a highly automatic manner. Here then       variables, rounded up to an integral
is the random forest analysis for the forensic glass data, leaving the  value.
tuning parameter (mtry) at its default5:
192 learning and exploring r
(fgl.rf <- randomForest(type ~ ., data=fgl))

Call:
 randomForest(formula = type ∼ ., data = fgl)
                           Type of random forest: classification
                                      Number of trees: 500

No. of variables tried at each split: 3

OOB estimate of error rate: 20.56%

Confusion matrix:

WinF WinNF Veh Con Tabl Head class.error

WinF 63  610 0 0     0.1000

WinNF 10 59 1 3 2 1  0.2237

Veh 8 3 6 0 0 0 0.6471

Con 0 3 0 9 0 1 0.3077

Tabl 0 2 0 0 7 0     0.2222

Head 1 2 0 0 0 26    0.1034

This improves substantially on the linear discriminant result. This
may happen because the explanatory variables have eﬀects that are
non-linear on a logit scale. The more likely reason is that there are
interaction eﬀects, perhaps of a relatively complicated kind, for
which the lda() analysis has not accounted.

    The predictive accuracy might be further improved by varying
the tuning parameter mtry from its default. See help(tuneRF)
for details of the function tuneRF() that is designed to assist in
ﬁndind=g the optimum choice of mtry.

9.5 ∗Ordination                                                          An ordination might alternatively be
                                                                         based on road travel times, or on air
From Australian road travel distances between cities and larger          travel times.
towns, can we derive a plausible “map”, or “ordination”, showing
the relative locations? The resulting “map” would give a better indi-    The ordination methods described here
cation than a geographical map of the road travel eﬀort involved in      are all versions of multi-dimensional
getting from one place to another.                                       scaling (MDS). If distances are not
                                                                         already given, a ﬁrst tasl is to calculate
    Genomic data provides another example. Various methods are           ‘distances’ between points. Or if
available for calculating genomic “distances ” between, e.g., diﬀer-     similarities are given, they must be
ent insect species. The distance measures are based on evolutionary      ﬁrst be transformed into ‘distances’.
models that aim to give distances between pairs of species that are a
monotone function of the time since the two species separated.           6 Principal components analysis cir-
                                                                         cumvents the calculation of distances,
    Ordination is a generic name for methods for providing a low-        for the commonly used Euclidean
dimensional view of points in multi-dimensional space, such that         distance measure. See below.
“similar” objects are near each other and dissimilar objects are sep-
arated. The plot(s) from an ordination in 2 or 3 dimensions may
provide useful visual clues on clusters in the data and/or on outliers.

    One standard type of problem starts from a matrix X of n obser-
vations by p variables, then seeking a low-dimensional represen-
tation. A ﬁrst step is then to calculate distances between observa-
tions.6 The hope is that a major part of the information content in
the p variables, as it relates to the separation between observations,
                                                                                       a miscellany of models & methods 193

can be pretty much summarized in a small number of constructed
variables.

    There is typically no good model, equivalent to the evolutionary
models used by molecular biologists, that can be used to motivate
distance calculations. There is then a large element of arbritariness
in the distance measure used. Results may depend strongly on the
distance measure used. Unless measurements are comparable (e.g.,
relative growth, as measured perhaps on a logarithmic scale, for
diﬀerent body measurements), it is usually desirable to calculate dis-
tances from standardized variable values. This is done by subtracting
the mean and dividing by the standard deviation.

    If data can be separated into known classes that should be re-
ﬂected in any ordination, then the scores from classiﬁcation using
lda() may be a good basis for an ordination. Plots in 2 or perhaps 3
dimensions may then reveal additional classes and/or identify points
that may be misclassiﬁed and/or are in some sense outliers. They
give an indication of the eﬀectiveness of the discrimination method
in choosing the boundaries between classes.

    Figure 9.13 demonstrated the use of “proximities” that are avail-
able from randomForest() as measures of the closeness of any pair
of points. These were then turned into rough distance measures that
then formed the basis for an ordination. With Support Vector Ma-
chines, distance measures can be derived from the ’decision values’
and used for ordination.

9.5.1 Distance measures

Euclidean distances

Treating the rows of X (n by p) as points in a p-dimensional space,    7 This says that a straight line is the
the squared Euclidean distance di2j between points i and j is          shortest distance between two points!

                                                  p                    8 More generally, they can be arbitrar-
                                                                       ily transformed before calculating the
                           di2j = (xik − x jk)2                        di j.

                                              k=1

The distances satisfy the triangle inequality7

                     di j ≤ dik + dk j

The columns may be weighted diﬀerently.8 Use of an un-

weighted measure with all columns scaled to a standard deviation

of one is equivalent to working with the unscaled columns and cal-

culating di2j as

                     p

                  di2j = wi j(xik − x jk)2

                     k=1

where wi j = (sis j)−1 is the inverse of the product of the standard
deviations for columns i and j.

    Where all elements of a column are positive, use of the logarith-
mic transformation is common. A logarithmic scale makes sense for
194 learning and exploring r

biological morphometric data, and for other data with similar char-
acteristics. For morphometric data, the eﬀect is to focus attention
on relative changes in the various body proportions, rather than on
absolute magnitudes.

Non-Euclidean distance measures                                            For the Manhattan distance:

Euclidean distance is one of many possible choices of distance mea-                                 p
sures, still satisfying the triangle inequality. As an example of a
non-Euclidean measure, consider the Manhattan distance. The Man-                     di j = | xik − x jk |
hattan distance is the shortest distance for a journey that always pro-
ceeds along one of the co-ordinate axes. In Manhattan in New York,                               k=1
streets are laid out in a rectangular grid. This is then (with k = 2) the
walking distance along one or other street. For other choices, see the     9 The function daisy() in the clus-
help page for the function dist().9                                        ter package oﬀers a wider choice,
                                                                           including distance measures for factor
From distances to a representation in Euclidean space                      or ordinal data. Its argument stand
                                                                           causes prior standardization of data.
Irrespective of the method of calculation of the distance measure,
ordination methods yield a representation in Euclidean space. It           10 This is true whether ot not the
is always possible to ﬁnd a conﬁguration X in Euclidean space in           triangle inequality is satisﬁed.
which the “distances” are approximated, perhaps rather poorly.10
It will become apparent in the course of seeking the conﬁguration
whether an exact embedding (matrix X) is possible, and how ac-
curate this embedding is. The representation is not unique. The
matrices X and XP, where P is an orthonormal matrix, give exactly
the same distances.

The connection with principal components                                   We assume that none of the columns
                                                                           can be written as a linear combination
Let X be an n by p matrix that is used for the calculation of Eu-          of other columns.
clidean distances, after any transformations and/or weighting. Then
metric p-dimensional ordination, applied to Euclidean distances be-
tween the rows of X, yields a representation in p-dimensional space
that is formally equivalent to that derived from the use of principal
components. The function cmdscale() yields, by a diﬀerent set of
matrix manipulations, what is essentially a principal components
decomposition. Principal components circumvents the calculation of
distances.

Semi-metric and non-metric scaling                                         The assumption of a Euclidean dis-
                                                                           tance scale is a convenient starting
Semi-metric and non-metric methods all start from “distances”,             point for calculations. An ordina-
but allow greater ﬂexibility in their use to create an ordination. The     tion that preserves relative rather
aim is to represent the “distances” in some speciﬁed number of             than absolute distances can often be
dimensions, typically two dimensions. As described here, a ﬁrst step       more appropriate. Additionally, small
is to treat the distances as Euclidean, and determine a conﬁguration       distances may be measured more
in Euclidean space. These Euclidean distances are then used as a           accurately than large distances.
                                                                                       a miscellany of models & methods 195

starting point for a representation in which the requirement that
these are Euclidean distances, all determined with equal accuracy,
is relaxed. The methods that will be noted here are Sammon scaling
and Kruskal’s non-metric multidimensional scaling.

Example – Australian road distances                                                  q

The distance matrix that will be used is in the matrix audists, from              Perth
the DAAG package. Figure 9.14 is from the use of classical multi-
dimensional scaling, as implemented in the function cmdscale():                                                                        Melbourne
An alternative way to add names of cities or other labels is to use
identify() to add labels interactively, thus:                                                                                          q q q Canberra
                                                                                                                                                      q
    Code is:                                                                                                                           Adelaide
                                                                                                                                                 Sydney
audists <- DAAG::audists
aupts <- cmdscale(audists)                                                     q                                                   q         q
plot(aupts, axes=FALSE, ann=FALSE, fg="gray",
                                                                          Broome                                                Alice  Brisbane
         frame.plot=TRUE)
city <- rownames(aupts)                                                                q
pos <- rep(1,length(city))
pos[city=="Melbourne"]<- 3                                                        Darwin
pos[city=="Canberra"] <- 4
par(xpd=TRUE)                                                                                                                              q
text(aupts, labels=city, pos=pos)
par(xpd=FALSE)                                                                                                                         Cairns

    Classical multi-dimensional scaling, as implemented by                Figure 9.14: Relative locations of
cmdscale(), gives long distances the same weight as short dis-            Australian cities, derived from road
tances. It is just as prepared to shift Canberra around relative to Mel-  map distances, using metric scaling.
bourne and Sydney, as to move Perth. It makes more sense to give
reduced weight to long distances, as is done by sammon() (MASS).

A: Using Classical MDS                                  q                B: Using Sammon Scaling                                        q

                                     q                                q                                       q                                       q
                    q                                             q                          q                                                    q
                                                              q                                                                               q
                                           q           q                                                            q                  q

        q                                                                        q
                                                    q                                                                        q

    Figure 9.15 shows side by side the overlays of the “maps” that re-    Figure 9.15: In Panel A, Figure 9.14
sult from the diﬀerent ordinations, onto a physical map of Australia.     has been linearly transformed, to give
Panel A shows the result for classical multi-dimensional scaling.         a best ﬁt to a map of Australia. Each
Panel B does the same, now for the result from Sammon scaling.            city moves as shown by the line that
                                                                          radiates out from it. Panel B is the
                                                                          equivalent plot for Sammon scaling.
196 learning and exploring r

Notice how Brisbane, Sydney, Canberra and Melbourne now main-
tain their relative positions better.

    The function oz::oz(), with default arguments, draws an out-
line of the coast of Australia, with state boundaries shown. Argu-
ments are available that can be used to limit what is shown (e.g. the
NSW and Victorian states only). To see the code for the overlays
onto the map of the Australian coast, source the ﬁle that has the code
for the ﬁgures of this chapter, and type:

fig9.15A
fig9.15B

    The exercise can be repeated for multidimensional scaling
(MDS). MDS preserves only, as far as possible, the relative dis-
tances. A starting conﬁguration of points is required. This might
come from the conﬁguration given by cmdscale(). For the sup-
plementary ﬁgure supp9.1() that shows the MDS conﬁguration,
however, we use the physical latitudes and longitudes.

    To show this ﬁgure, source the ﬁle, if this has not been done
previously, that has the code for the ﬁgures of this chapter. Then
type:

supp9.1()

9.6 Maps, map overlays, and spatial analysis

Extensive information is available both in vignettes included with R
package, and on online. Code that is provided with the vignettes can
be used to recreate the maps.

  Lovelace et al.(2014) is a tutorial overview.
  The mapmisc vignette "Overview of mapping with mapmisc"
  shows a number of maps that illustrate some of the possibilities.
  Helpful sources of information on the relevant R data structures
  are the sf vignette "Simple Features for R" and the tmap vignette
  "tmap in a nutshell."
  Detailed information on relevant R packages and abilities is
  available under Spatial on the R Task Views web page (http:
  //cran.ms.unimelb.edu.au/web/views/Spatial.html).
  See articles in the Journal of Statistical Software special volume
  https://www.jstatsoft.org/issue/view/v063 (Volume 6e3, 2015.)

References

  Cowpertwait and Metcalfe 2009. Introductory Time Series with R.
  Springer.
  Hyndman et al. 2008. Forecasting with Exponential Smoothing:
  The State Space Approach. 2nd edn.
                                                                                    a miscellany of models & methods 197

Lovelave and Cheshire 2014. Introduction to visualising spa-
tial data in R. National Centre for Research Methods Work-
ing Papers, 14(03). Retrieved from https://github.com/
Robinlovelace/Creating-maps-in-R.

Taleb 2004. Fooled By Randomness: The Hidden Role Of Chance
In Life And In The Markets. Random House, 2ed.
[Has insightful comments on the over-interpretation of phenomena
in which randomness is likely to have a large role.]
198 learning and exploring r
