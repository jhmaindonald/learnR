---
title: "Functions for Index generation"
author: "John Maindonald"
date: "11/9/2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

To get a pdf and hence text file in which the listings (in color)
code can be searched for names, remove the `%` fromm the start of
the line `% columns=fullflexible,` in Sweavel.sty, before running
`pdflatex learnR`

```{r makeTxt}
## In this document, numbering, starts at page 1.
txpages <- pdftools::pdf_text('learnR.pdf')
nlast <- grep('^Index\ of\ Functions',txpages) - 1 
txpages <- txpages[1:nlast]
contpg <- grep('^[[:blank:]]*Contents',txpages)
conts <- paste(c(txpages[contpg],sub('^[[:blank:]]*[[:digit:]]+\n','',txpages[contpg+1])), collapse="")
conts <- strsplit(sub('^[[:blank:]]*Contents\n','',conts), '\n')[[1]]
newchaps <- gsub('[^[:digit:]]+[[:digit:]]?[^[:digit:]]+',':', conts)
chapinfo <- t(sapply(strsplit(newchaps,':'),function(x)as.numeric(x)))
# writeLines(txpages[chapinfo[1,2]:(chapinfo[2,2]-1)], '~/pkgs/DAAGviz/inst/texts/1getStarted.txt')
# writeLines(txpages[chapinfo[2,2]:(chapinfo[3,2]-1)], '~/pkgs/DAAGviz/inst/texts/2workenv.txt')
# writeLines(txpages[chapinfo[3,2]:(chapinfo[4,2]-1)], '~/pkgs/DAAGviz/inst/texts/3exs.txt')
# writeLines(txpages[chapinfo[4,2]:(chapinfo[7,2]-1)], '~/pkgs/DAAGviz/inst/texts/456data.txt')
```

```{bash look4funs}
## Look in the scripts subdirectory for packages
## Output to scriptPkg.txt
egrep -ho 'library\([[:alpha:]]+[[:digit:]]?)|[[:alpha:]]+[[:digit:]]?::|\{chapter}\{[[:digit:]]+}' scripts/*R | sed 's/library(//g' | sed 's/)//g' |  sed 's/:://g' > scriptPkg.txt
```

```{bash look4pkg}
egrep -ho 'library\([[:alpha:]]+[[:digit:]]?)|[[:alpha:]]+[[:digit:]]?::|\{chapter}\{[[:digit:]]+}' learnR.txt | sed 's/library(//g' | sed 's/)//g' |  sed 's/:://g' > txtPkg.txt
```


```{r indexFuns}
showpkg <-
function (pkgs = c("base", "car", "DAAG", "DAAGviz", "datasets", "dplyr", "Ecdat", 
"forecast", "gamclass", "ggplot2", "googleVis", "graphics", "grid", 
"HistData", "knitr", "lattice", "latticeExtra", "lme4", "MASS", 
"memisc", "mgcv", "oz", "plyr", "quantreg", "Rcmdr", "RColorBrewer", 
"readr", "readxl", "reshape2", "rgdal", "rgl", "rpart", "RSQLite", 
"sp", "splines", "tm", "vcd", "WDI", "wordcloud", "XML")) 
{
# pkgs <- sort(unique(c(sort(read.table('scriptPkg.txt',as.is=TRUE)),
#              sort(read.table('txtPkg.txt',as.is=TRUE)))))
for (i in 1:length(pkgs)) require(pkgs[i], warn.conflicts = FALSE, character.only = TRUE,quietly=TRUE)
  where <- search()
  if (length(where) < 1L) 
    stop("argument 'where' of length 0")
  ispkg <- substring(where, 1, 8) == "package:"
  where <- where[ispkg]
  z <- vector(length(where), mode = "list")
  newnam <- sapply(strsplit(where, ":", fixed = TRUE), function(x) x[2])
  names(z) <- newnam
  for (i in seq_along(where)) z[[i]] <- lsf.str(pos = where[i], all.names=TRUE)
  z <- sapply(names(z), function(x) {
    funs <- z[[x]]
    paste(rep(x, length(funs)), funs, sep = ":")
  })
  all <- unlist(z, use.names = FALSE)
  zz <- strsplit(all, ":")
  funtab <- cbind(sapply(zz, function(x) x[1]), sapply(zz, 
                                                       function(x) x[2]))
  funtab <- rbind(funtab, c('base','.First'), c('base','.Last'))
  funtab
}
```

```{r locatefun}
locatefun <-
function (txpg, notfun)
{
idtxt <- "\\.?[a-zA-Z][a-zA-Z0-9]*([._][a-zA-Z]+)*\\("
page <- 1:length(txpg)
byPage <- gregexpr(idtxt, txpg)
mn <- lapply(byPage,function(x)cbind(x,x+attr(x, "match.length")-2))
namlist <- lapply(1:length(mn),function(k)substring(txpg[[k]],mn[[k]][,1],mn[[k]][,2]))
num <- sapply(namlist,length)
xy <- data.frame(nam=unlist(namlist), page=rep(1:length(namlist),num))
here <- !(xy[, 1] %in% c("",notfun))
xy[here,]
}
```

```{r mfi}
makeFunIndex <-
function (sourceFile = "learnR.pdf", frompath = "~/_notes/learnR",
topath = NULL, fileout = NULL, availfun = funpack,
offset = 0, notfun=c("", "al", "Pr", "T", "F", "n", "P",
"y", "A", "\\n", "transformation", "left", "f", "site.",
"a", "b", "II", "ARCH", "ARMA", "MA", "AR", "R_"))
{
len <- nchar(sourceFile)
lfrom <- nchar(frompath)
if (substring(frompath, lfrom, lfrom) == "/")
frompath <- substring(frompath, 1, lfrom - 1)
if (is.null(fileout)) {
if (substring(sourceFile, len - 3, len - 3) == ".")
fnam <- substring(sourceFile, 1, len - 4)
else fnam <- sourceFile
fileout <- paste(fnam, ".fdx", sep = "")
fdxfile <- paste(fileout, sep = "/")
fndfile <- paste(fnam, ".fnd", sep = "")
}
if (is.null(topath))
topath <- frompath
else {
lto <- nchar(topath)
if (substring(topath, lto, lto) == "/")
topath <- substring(topath, 1, lto - 1)
}
sourceFile <- paste(frompath, sourceFile, sep = "/")
print(paste("Send output to", fndfile))
tx <- pdftools::pdf_text('learnR.pdf')
nlast <- grep('^Index\ of\ Functions',tx) - 1
tx <- tx[1:(nlast-1)]
tx <- gsub(' (', '(', tx, fixed=TRUE)
entrymat <- locatefun(tx, notfun=notfun)
backn <- regexpr("\\n", entrymat[, 1], fixed = TRUE)
entrymat <- entrymat[backn < 0, ]
entrymat[, 2] <- entrymat[, 2] - offset
nmatch <- match(entrymat[, 1], availfun[, 2], nomatch = 0)
use <- nmatch > 0
print("Unmatched functions:")
print(unique(entrymat[!use, 1]))
entrymat <- entrymat[use,]
entrymat[, 1] <- gsub("_", "\\_", entrymat[, 1], fixed = TRUE)
nmatch <- nmatch[use]
entrymat[, 1] <- paste(entrymat[, 1], " ({\\em ", availfun[nmatch,
1], "})", sep = "")
funentries <- paste("\\indexentry  ", "{", entrymat[, 1],
"}{", entrymat[, 2], "}", sep = "")
write(funentries, fdxfile)
system(paste("makeindex -c -o", fndfile, fdxfile))
## system(paste("mv", fndfile, topath))
}
```

```{r mkFnIdx}
## Set offset to # of preamble pages
## Create fdx, then fnd file
pkgNam <- unique(c(scan('scriptPkg.txt', what=''),scan('txtPkg.txt', what='')))
pkgNam <- pkgNam[-match("Rcmdr",pkgNam)]
funpack <- showpkg(pkgs=pkgNam)
makeFunIndex(offset=0, availfun=funpack)
```

#######################################################################

```{r termInd}
## offset=0 because page numbers start at the first page of the pdf
## makeTermIndex() calls locateTerm()

makeTermIndex <-
function (sourceFile = "learnR.pdf", offset = 0, idxFile = "learnR.idx",
    indFile = "learnR.ind", termMat=tmat4, ignore.case=TRUE, limto=555)
{
    len <- nchar(sourceFile)
    if (substring(sourceFile, len - 3, len - 3) == ".")
            fnam <- substring(sourceFile, 1, len - 4)
        else fnam <- sourceFile
    tmpFile <- paste0(fnam, ".tmp")
    if (is.null(idxFile)) idxFile <- paste0(fnam, ".idx")
    if (is.null(indFile)) indFile <- paste0(fnam, ".ind")
    tx <- pdftools::pdf_text(sourceFile)
    nlast <- grep('^Index\ of\ Functions',tx) - 1
    tx <- tx[1:(nlast-1)]
    tx <- gsub('â€“','-',tx)
    indentry <- locateTerm(tx, termMat, offset = offset, ignore.case=ignore.case, limto=limto)
    nopage <- sapply(indentry, function(x)x[1]==999)
    indIN <- indentry[!nopage]
    indOUT <- indentry[nopage]
    namIN <- names(indIN)
    namOUT <- names(indOUT)
    termentriesIN <- unlist(sapply(1:length(namIN), function(k) paste("{",
        namIN[k], "}{", indIN[[k]], "}", sep = "")))
    termentriesIN <- paste("\\indexentry", termentriesIN, sep = "")
    termcon <- file(idxFile)
    write(termentriesIN, file = idxFile)
    close(termcon)
#    system(paste("makeindex -c", idxFile))
    system(paste("makeindex -c -o", tmpFile, idxFile))
    system(paste("sed 's/{1}/{}/g'", tmpFile, ">", indFile))
if(any(nopage)){
    noEntry <- unlist(sapply(1:length(indOUT), function(k) paste(
        namOUT[k], indOUT[[k]], sep = "")))
    noEntry <- gsub('\\tt','',noEntry,fixed=TRUE)
    noEntry <- gsub('999','',noEntry,fixed=TRUE)
    noEntry <- gsub('\\itshape','',noEntry,fixed=TRUE)    
    noEntry <- gsub('\\texttt','',noEntry,fixed=TRUE)
    noEntry <- gsub('\\','',noEntry,fixed=TRUE)}
    else(noEntry<-NULL)
    invisible(noEntry)
}

locateTerm <-
function (txpg = DAAGURtx, tmat = termMat, offset = 0, ignore.case,
    limto = NA)
{
    idtxt <- "[a-zA-Z][a-zA-Z0-9\\.\\_]*?\\("
    txpg <- gsub("[[\\]]", " ", txpg)
    txpg <- gsub(". ", " ", txpg, fixed = TRUE)
    txpg <- gsub("\\([^\\)]|[^\\(]\\)", " ", txpg)
    txpg <- gsub("[^0-9a-zA-Z)(\f _.-]", "", txpg)
#    txpg <- tolower(txpg)
#    for(j in 2:5)tmat[,j]  <- tolower(tmat[,j])
    word1 <- tmat[, 2]
    word2 <- tmat[, 3]
    altwd1 <- tmat[, 4]
    altwd2 <- tmat[, 5]
    nterm <- length(word1)
    count <- numeric(nterm)
    page <- 1:length(txpg)
    cat("\nPage range is:", min(page), "to", max(page), "\n")
    if (offset >= 1)
        txpg <- txpg[-(1:offset)]
    if (!is.na(limto) & limto < length(txpg))
        txpg <- txpg[1:limto]
    k <- 0
    n1 <- 1:length(txpg)
    indentry <- vector("list", length(terms))
    for (k in 1:nterm) {
        tm1 <- word1[k]
#       tm1 <- paste("[[:space:][:punct:]]?", word1[k], "[[:space:][:punct:]]?", sep = "")
        # tm1a <- paste(" ", word1[k], ",", sep = "")
        # tm1b <- paste(" ", word1[k], ".", sep = "")
        tm2 <- word2[k]
        alt1 <- altwd1[k]
        alt2 <- altwd2[k]
        regr1 <- regexpr(tm1, txpg, fixed=TRUE)
        # regr1a <- regexpr(tm1a, txpg, fixed = TRUE)
        # regr1b <- regexpr(tm1b, txpg, fixed = TRUE)
        here <- regr1 > 0      
        # here <- regr1 > 0 | regr1a > 0 |regr1b > 0
        if (!is.na(alt1)&(tm1!="zzzz")) {
#            alt1 <- paste("[[:space:][:punct:]]?", alt1, "[[:space:][:punct:]]?", sep = "")
            altregr1 <- regexpr(alt1, txpg,fixed=TRUE)
            here <- here | altregr1 > 0
        }
        if (!is.na(tm2)) {
#            tm2 <- paste("[[:space:][:punct:]]?", tm2, "[[:space:][:punct:]]?", sep = "")
            regr2 <- regexpr(tm2, txpg, fixed=TRUE)
            here2 <- regr2 > 0
            if (!is.na(alt2)) {
#                alt2 <- paste("[[:space:][:punct:]]?", alt2, "[[:space:][:punct:]]?", sep = "")
                altregr2 <- regexpr(alt2, txpg,fixed=TRUE)
                here2 <- here2 | altregr2 > 0
            }
            here <- here & here2
        }
        pg <- n1[here]
        if (length(pg) == 0) {
                      if(tm1 == "zzzz") indentry[k] <- list(1) else
            indentry[k] <- list(999)
        }
        else indentry[k] <- list(pg)
        count[k] <- sum(here)
    }
    names(indentry) <- tmat[, 1]
    indentry
}

# showTermPage <-
# function (sourceFile = "~/r3/proofs/r-mono.txt", tmat = termMat,
#     offset = 0, fileout = "~/r3/proofs/bypage.txt", indentry = NULL)
# {
#     if (is.null(indentry)) {
#         len <- nchar(sourceFile)
#         tx <- readLines(sourceFile, warn = FALSE)
#         indentry <- locateTerm(tx, termMat, offset = offset)
#     }
#     tnam <- names(indentry)
#     len <- length(tnam)
#     itemlen <- sapply(indentry, length)
#     tlist <- sapply(1:len, function(k) cbind(indentry[[k]], rep(k,
#         itemlen[k])))
#     t1 <- unlist(lapply(tlist, function(x) x[, 1]))
#     t2 <- unlist(lapply(tlist, function(x) x[, 2]))
#     termlist <- split(t2, t1)
#     pnam <- names(termlist)
#     write("Page List", file = fileout)
#     for (i in 1:length(termlist)) {
#         ntms <- unique(termlist[[i]])
#         ntms <- ntms[ntms != 999]
#         if (length(ntms) > 0) {
#             tms <- sort(tnam[ntms])
#             tms <- sub("!", "!! ", tms, fixed = TRUE)
#             tmsplit <- strsplit(tms, "!!", fixed = T)
#             tx1 <- sapply(tmsplit, function(x) x[1])
#             tx2 <- sapply(tmsplit, function(x) if (length(x) >
#                 1)
#                 x[2]
#             else "")
#             z <- split(tx2, tx1)
#             nam <- names(z)
#             chvec <- unlist(as.vector(sapply(1:length(z), function(i) c(nam[i],
#                 z[[i]]))))
#             write(pnam[i], file = fileout, append = TRUE)
#             write(chvec, file = fileout, append = TRUE)
#             write(" ", file = fileout, append = TRUE)
#         }
#     }
#     unlink(fileout)
# }
```

#######################################################################

```{r mkind}
## Create idx, then ind file
tmat4 <- read.table('~/_notes/learnR/tmat4.txt', as.is=TRUE)
tmat4 <- tmat4[-c(1,29,31,35:36,295:297,546:547),]
notIn <- makeTermIndex(termMat=tmat4, offset=0)
```

<!-- ```{bash idxTOind} -->
<!-- makeindex -c draft.idx -->
<!-- ``` -->

```{r tokens}
tmat4 <- read.table('tmat4.txt', as.is=TRUE)
tx <- readLines('draft.txt')
eol <- regexpr('\f',tx)
pg <- cumsum(eol==1)+1
txword <-  tokenizers:: tokenize_words(txx, stopwords=tm::stopwords('SMART')[-c(147, 275, 288, 491, 504)],lowercase=FALSE)
z2 <- sapply(tmat4[,2],
            function(x)sapply(txword,function(w)match(x,w,nomatch=0),
                              USE.NAMES=TRUE),USE.NAMES=TRUE)
z3 <- sapply(tmat4[,3],
            function(x)sapply(txword,function(w)match(x,w,nomatch=0),
                              USE.NAMES=TRUE),USE.NAMES=TRUE)
u2 <- apply(z2,2,sum)
nam2 <- unique(names(u2))
tag2 <- gsub(' ', '-',nam2,fixed=TRUE) 
u3 <- apply(z3,2,sum)
nam3 <- names(u3)
```
